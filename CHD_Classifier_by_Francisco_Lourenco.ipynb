{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a98f73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR, LossCustom\n",
    "from models.segmentation import DETRsegm, PostProcessPanoptic\n",
    "from models.matcher import HungarianMatcher\n",
    "from hubconf import detr_resnet101_panoptic, detr_resnet3d_panoptic\n",
    "from torchvision.transforms import Resize\n",
    "from util.misc import get_world_size\n",
    "import open3d as o3d\n",
    "import torch.optim as optim\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e667078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.imageglobals.logger.level = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, target):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(target)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25ca9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_downsample_image(image, down_scale = 8):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], down_scale)\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], down_scale)\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], down_scale)\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b1ff6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_image_to_given_size(image, size = 128):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], int(image_shape[0]/size))\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], int(image_shape[1]/size))\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], int(image_shape[2]/size))\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e187e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmented_image(segmented_image):\n",
    "#     segmented_image.squeeze()\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    colors = [[128,128,128],[255,0,0],[255,255,0],[0,255,0],[0,255,255],[0,0,255],[255,0,255]]\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    for _class in classes[1:]:\n",
    "        points_numpy = (segmented_image == _class).nonzero().numpy()\n",
    "        o3d_point_cloud = o3d.geometry.PointCloud()\n",
    "        o3d_point_cloud.points = o3d.utility.Vector3dVector(points_numpy)\n",
    "        o3d_point_cloud.estimate_normals()\n",
    "        o3d_point_cloud.paint_uniform_color(np.array(colors[int(_class)-1])/255)\n",
    "        vis.add_geometry(o3d_point_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c45f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    bb_list = []\n",
    "    for class_ in classes[1:]: \n",
    "        points = (segmented_image == class_).nonzero()\n",
    "        x_min, x_max = points[:,0].min(), points[:,0].max()\n",
    "        y_min, y_max = points[:,1].min(), points[:,1].max()\n",
    "        z_min, z_max = points[:,2].min(), points[:,2].max()\n",
    "        bb = torch.tensor([(x_max-x_min)/2, (y_max-y_min)/2, (z_max-z_min)/2, x_max-x_min, y_max-y_min, z_max-z_min])\n",
    "        bb[0::2] = bb[0::2]/torch.tensor(segmented_image.shape)\n",
    "        bb[1::2] = bb[1::2]/torch.tensor(segmented_image.shape)\n",
    "        bb_list.append(bb)\n",
    "    bbs = torch.stack(bb_list)\n",
    "    return bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cc5678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    masks_list = []\n",
    "    for class_ in classes[1:]:\n",
    "        mask = segmented_image.clone()\n",
    "        mask[segmented_image == class_] = 1\n",
    "        mask[segmented_image != class_] = 0\n",
    "        masks_list.append(mask.short())\n",
    "    masks = torch.stack(masks_list)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dd321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(segmented_image):\n",
    "    return segmented_image.unique().long()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8630212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_dict_panoptic(segmented_image):\n",
    "    labels = get_labels(segmented_image)\n",
    "    boxes = get_bounding_boxes(segmented_image)\n",
    "    masks = get_masks(segmented_image)\n",
    "    target_dict = {'labels': labels, 'boxes': boxes, 'masks': masks, 'seg_im': segmented_image}\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2b3d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_dict_detection(segmented_image):\n",
    "    labels = get_labels(segmented_image)\n",
    "    boxes = get_bounding_boxes(segmented_image)\n",
    "    target_dict = {'labels': labels, 'boxes': boxes}\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d489865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_corners(box):\n",
    "    x_min = box[0] - box[3]/2\n",
    "    x_max = box[0] + box[3]/2\n",
    "    y_min = box[1] - box[4]/2\n",
    "    y_max = box[1] + box[4]/2\n",
    "    z_min = box[2] - box[5]/2\n",
    "    z_max = box[2] + box[5]/2\n",
    "    return torch.tensor([x_min,y_min,z_min,x_max,y_max,z_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a57447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersetion_box_corners(box_1,box_2):\n",
    "    intersection_box_corners = torch.zeros(6)\n",
    "    intersection_box_corners[0] = max(box_1[0],box_2[0])\n",
    "    intersection_box_corners[1] = min(box_1[3],box_2[3])\n",
    "    intersection_box_corners[2] = max(box_1[1],box_2[1])\n",
    "    intersection_box_corners[3] = min(box_1[4],box_2[4])\n",
    "    intersection_box_corners[4] = max(box_1[2],box_2[2])\n",
    "    intersection_box_corners[5] = min(box_1[5],box_2[5])\n",
    "    return intersection_box_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74035a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_box_volume(box):\n",
    "    box_volume = (box[3]-box[0])*(box[4]-box[1])*(box[5]-box[2])\n",
    "    if box_volume < 0: box_volume = 0\n",
    "    return box_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c704946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_box_iou(box_1,box_2):\n",
    "    box_1_corners = get_box_corners(box_1)\n",
    "    box_2_corners = get_box_corners(box_2)\n",
    "    intersection_box = get_intersetion_box_corners(box_1_corners,box_2_corners)\n",
    "    intersection_box_volume = compute_box_volume(intersection_box)\n",
    "    if intersection_box_volume == 0: return 0\n",
    "    box_1_volume = compute_box_volume(box_1_corners) \n",
    "    box_2_volume = compute_box_volume(box_2_corners)\n",
    "    iou = intersection_box_volume/(box_1_volume+box_2_volume-intersection_box_volume)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e1eed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_precision(outputs, target, labels):\n",
    "    ious = []\n",
    "    ap = []\n",
    "    for i, label in enumerate(labels):\n",
    "        pred_box = outputs['pred_boxes'].squeeze()[i]\n",
    "        target_box = target['boxes'][label] \n",
    "        iou = compute_box_iou(pred_box,target_box)\n",
    "        ious.append(iou)\n",
    "    ious_tensor = torch.tensor(ious)\n",
    "    for t in range(50,95,5):\n",
    "        tp = (ious_tensor >= t/100).count_nonzero()\n",
    "        fn = abs(len(target['labels']) - 1 - tp)\n",
    "        fp = abs((outputs['pred_logits'].squeeze().argmax(1) != 0).count_nonzero() - 1 - tp)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        ap.append(precision*recall)\n",
    "    ap_tensor = torch.tensor(ap)\n",
    "    return ap_tensor.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa552c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_panoptic_quality(pred_seg, target):\n",
    "    ious = []\n",
    "    for i, label in enumerate(target['labels'][1:]):\n",
    "        target_mask = target['masks'][i] \n",
    "        pred_mask = (pred_seg == label).int()\n",
    "        pred_mask[pred_mask == 0] = -1\n",
    "        intersection = (pred_mask == target_mask).count_nonzero()\n",
    "        union = pred_mask[pred_mask == 1].count_nonzero() + target_mask[target_mask == 1].count_nonzero() - intersection\n",
    "        iou = intersection/union\n",
    "        ious.append(iou)\n",
    "    ious_tensor = torch.tensor(ious)\n",
    "    tp = (ious_tensor > 0.5).count_nonzero()\n",
    "    fn = abs(len(target['labels']) - 1 - tp)\n",
    "    fp = abs(len(pred_seg.unique()) - 1 - tp)\n",
    "    if tp > 0:\n",
    "        sq = ious_tensor[ious_tensor > 0.5].sum() / tp\n",
    "        rq = rq = tp / (tp + fp/2 + fn/2)\n",
    "        pq = sq*rq\n",
    "    else:\n",
    "        sq = 0\n",
    "        rq = 0\n",
    "        pq = 0\n",
    "    return pq, sq, rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cbf87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForDetection(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, target_paths, down_scale=8):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.down_scale = down_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(self.image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array).float()\n",
    "        downsampled_image = uniform_downsample_image(image_torch_tensor, down_scale=self.down_scale)\n",
    "        downsampled_image = downsampled_image/downsampled_image.max()\n",
    "#         downsampled_image = downsample_image_to_given_size(image_torch_tensor, size=64)\n",
    "        target_np_array = nib.load(self.target_paths[i]).get_fdata()\n",
    "        target_torch_tensor = torch.from_numpy(target_np_array)\n",
    "        target_torch_tensor[target_torch_tensor > 7] = 0\n",
    "        downsampled_target = uniform_downsample_image(target_torch_tensor, down_scale=self.down_scale)\n",
    "#         downsampled_target = downsample_image_to_given_size(target_torch_tensor, size=64)\n",
    "        target_dict = create_target_dict_detection(downsampled_target)\n",
    "        return downsampled_image, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a11d132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, target_paths, down_scale=8):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.down_scale = down_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(self.image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array).float()\n",
    "        downsampled_image = uniform_downsample_image(image_torch_tensor, down_scale=self.down_scale)\n",
    "        downsampled_image = downsampled_image/downsampled_image.max()\n",
    "#         downsampled_image = downsample_image_to_given_size(image_torch_tensor, size=64)\n",
    "        target_np_array = nib.load(self.target_paths[i]).get_fdata()\n",
    "        target_torch_tensor = torch.from_numpy(target_np_array)\n",
    "        target_torch_tensor[target_torch_tensor > 7] = 0\n",
    "        downsampled_target = uniform_downsample_image(target_torch_tensor, down_scale=self.down_scale)\n",
    "#         downsampled_target = downsample_image_to_given_size(target_torch_tensor, size=64)\n",
    "        target_dict = create_target_dict_panoptic(downsampled_target)\n",
    "        return downsampled_image, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6625d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dset_path, dset_type, split_index=87, down_scale=10):\n",
    "    image_paths = glob(f'{dset_path}/*image.nii.gz',recursive=True)\n",
    "    target_paths = glob(f'{dset_path}/*label.nii.gz',recursive=True)\n",
    "    image_paths.sort(key=sort_func)\n",
    "    target_paths.sort(key=sort_func)\n",
    "    if dset_type == 'detection':\n",
    "        train_dset = DatasetForDetection(image_paths[:split_index], target_paths[:split_index], down_scale=down_scale)\n",
    "        valid_dset = DatasetForDetection(image_paths[split_index:],target_paths[split_index:], down_scale=down_scale)\n",
    "    if dset_type == 'panoptic':\n",
    "        train_dset = DatasetForSegmentation(image_paths[:split_index], target_paths[:split_index], down_scale=down_scale)\n",
    "        valid_dset = DatasetForSegmentation(image_paths[split_index:], target_paths[split_index:], down_scale=down_scale)\n",
    "    return train_dset, valid_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16883bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndEvaluate():\n",
    "    def __init__(self, model, optimizer, loss, train_dset, valid_dset):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.train_dset = train_dset\n",
    "        self.valid_dset = valid_dset\n",
    "        self.matcher = loss.matcher \n",
    "        aux_target = train_dset[0][1]\n",
    "        if 'masks' in aux_target.keys():\n",
    "            self.masks = True\n",
    "            self.key = 'loss_mask'\n",
    "            is_thing_map = {'0': False, '1': True, '2': True, '3': True, '4': True, '5': True, '6': True, '7': True}\n",
    "            self.post_process_panoptic = PostProcessPanoptic(is_thing_map)\n",
    "        else: \n",
    "            self.masks = False\n",
    "            self.key = 'loss_bbox'        \n",
    "    \n",
    "    def train_and_evaluate(self, n_epoch, save=False):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            kbar = pkbar.Kbar(target=(len(self.train_dset)+len(self.valid_dset)-2), epoch=epoch, num_epochs=n_epoch, width=16)\n",
    "            running_loss_t = 0.0\n",
    "            running_loss_v = 0.0\n",
    "            rdm = torch.randperm(len(self.train_dset))\n",
    "            \n",
    "            for i, j in enumerate(rdm):\n",
    "                b = self.train_dset[j]\n",
    "                image = b[0]\n",
    "                image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "                target = b[1]\n",
    "                for t in target.items():\n",
    "                    target[t[0]] = t[1].to(device)\n",
    "                outputs = self.model(image)\n",
    "                self.optimizer.zero_grad()\n",
    "                if self.masks:\n",
    "                    l = self.loss.loss_masks(outputs, [target]) #To train masks\n",
    "                else:\n",
    "                    l = self.loss.loss_boxes(outputs, [target]) #To train boxes\n",
    "                l[self.key].backward() \n",
    "                self.optimizer.step()\n",
    "                running_loss_t = l[self.key].item()\n",
    "                kbar.update(i, values=[(\"Train Loss\", running_loss_t)])\n",
    "            \n",
    "            if save:\n",
    "                torch.save(self.model.state_dict(), '/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/checkpoint_detr_hd.pth')\n",
    "            \n",
    "            kbar.add(1, values=[(\"Validation Loss\", 0), (\"Accuracy\", 0)])        \n",
    "            with torch.no_grad():\n",
    "                for ii, b in enumerate(self.valid_dset):\n",
    "                    image = b[0]\n",
    "                    image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "                    target = b[1]\n",
    "                    for t in target.items():\n",
    "                        target[t[0]] = t[1].to(device)\n",
    "\n",
    "                    outputs = self.model(image)\n",
    "                    if self.masks:\n",
    "                        l = self.loss.loss_masks(outputs, [target]) #To train masks\n",
    "                    else:\n",
    "                        l = self.loss.loss_boxes(outputs, [target]) #To train boxes\n",
    "                    running_loss_v = l[self.key].item()\n",
    "                    src_idx, trgt_idx = self.matcher(outputs,[target])[0]\n",
    "                    if self.masks:\n",
    "                        pred_seg = post_process_panoptic(outputs[trgt_idx],[tuple(torch.tensor(b[0].shape).tolist())]).to(device)\n",
    "                        pq,sq,rq = compute_panoptic_quality(pred_seg, target)\n",
    "                        kbar.update(i+ii, values=[(\"Validation Loss\", running_loss_v), (\"PQ\", 100*pq),(\"SQ\", 100*sq),(\"RQ\", 100*rq)])\n",
    "                    else:\n",
    "                        ap = compute_average_precision(outputs, target, trgt_idx) #For boxes\n",
    "                        kbar.update(i+ii, values=[(\"Validation Loss\", running_loss_v), (\"Accuracy\", 100*ap)]) #For boxes\n",
    "        print('Finished Training')\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = '/home/francisco/workspace/ImageCHD_dataset'\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da2874c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = detr_resnet3d_panoptic()\n",
    "model = model.detr #To train boxes\n",
    "# model.load_state_dict(torch.load('/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/checkpoint_detr.pth'))\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f5ba723",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher()\n",
    "loss = LossCustom(matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae78ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset, valid_dset = create_dataset(path, 'detection', 87, 4)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00001, weight_decay=10e-4)\n",
    "trainer = TrainAndEvaluate(model, optimizer, loss, train_dset, valid_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0efa0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "108/108 [================] - 180s 2s/step - Train Loss: 0.6545 - Validation Loss: 0.6535 - Accuracy: 0.0000e+00\n",
      "Epoch: 2/100\n",
      "108/108 [================] - 189s 2s/step - Train Loss: 0.6630 - Validation Loss: 0.6450 - Accuracy: 0.0000e+00\n",
      "Epoch: 3/100\n",
      "108/108 [================] - 184s 2s/step - Train Loss: 0.6562 - Validation Loss: 0.6690 - Accuracy: 0.0000e+00\n",
      "Epoch: 4/100\n",
      "108/108 [================] - 194s 2s/step - Train Loss: 0.6560 - Validation Loss: 0.6708 - Accuracy: 0.0000e+00\n",
      "Epoch: 5/100\n",
      "108/108 [================] - 187s 2s/step - Train Loss: 0.6618 - Validation Loss: 0.6799 - Accuracy: 0.0000e+00\n",
      "Epoch: 6/100\n",
      "108/108 [================] - 194s 2s/step - Train Loss: 0.6481 - Validation Loss: 0.6394 - Accuracy: 0.0000e+00\n",
      "Epoch: 7/100\n",
      "108/108 [================] - 193s 2s/step - Train Loss: 0.6506 - Validation Loss: 0.6418 - Accuracy: 0.0000e+00\n",
      "Epoch: 8/100\n",
      "108/108 [================] - 199s 2s/step - Train Loss: 0.6486 - Validation Loss: 0.6589 - Accuracy: 0.0000e+00\n",
      "Epoch: 9/100\n",
      "108/108 [================] - 187s 2s/step - Train Loss: 0.6390 - Validation Loss: 0.6536 - Accuracy: 0.0000e+00\n",
      "Epoch: 10/100\n",
      "108/108 [================] - 186s 2s/step - Train Loss: 0.6558 - Validation Loss: 0.6590 - Accuracy: 0.0000e+00\n",
      "Epoch: 11/100\n",
      "108/108 [================] - 195s 2s/step - Train Loss: 0.6388 - Validation Loss: 0.6789 - Accuracy: 0.0000e+00\n",
      "Epoch: 12/100\n",
      "108/108 [================] - 189s 2s/step - Train Loss: 0.6435 - Validation Loss: 0.6425 - Accuracy: 0.0000e+00\n",
      "Epoch: 13/100\n",
      "108/108 [================] - 185s 2s/step - Train Loss: 0.6435 - Validation Loss: 0.6544 - Accuracy: 0.0000e+00\n",
      "Epoch: 14/100\n",
      "108/108 [================] - 196s 2s/step - Train Loss: 0.6473 - Validation Loss: 0.6406 - Accuracy: 0.0000e+00\n",
      "Epoch: 15/100\n",
      "108/108 [================] - 190s 2s/step - Train Loss: 0.6399 - Validation Loss: 0.6639 - Accuracy: 0.0000e+00\n",
      "Epoch: 16/100\n",
      "108/108 [================] - 181s 2s/step - Train Loss: 0.6415 - Validation Loss: 0.6439 - Accuracy: 0.0000e+00\n",
      "Epoch: 17/100\n",
      "108/108 [================] - 181s 2s/step - Train Loss: 0.6475 - Validation Loss: 0.6274 - Accuracy: 0.0000e+00\n",
      "Epoch: 18/100\n",
      "108/108 [================] - 176s 2s/step - Train Loss: 0.6471 - Validation Loss: 0.6633 - Accuracy: 0.0000e+00\n",
      "Epoch: 19/100\n",
      "108/108 [================] - 176s 2s/step - Train Loss: 0.6519 - Validation Loss: 0.6533 - Accuracy: 0.0000e+00\n",
      "Epoch: 20/100\n",
      "108/108 [================] - 189s 2s/step - Train Loss: 0.6355 - Validation Loss: 0.6398 - Accuracy: 0.0000e+00\n",
      "Epoch: 21/100\n",
      "108/108 [================] - 171s 2s/step - Train Loss: 0.6478 - Validation Loss: 0.6577 - Accuracy: 0.0000e+00\n",
      "Epoch: 22/100\n",
      "108/108 [================] - 176s 2s/step - Train Loss: 0.6395 - Validation Loss: 0.6659 - Accuracy: 0.0000e+00\n",
      "Epoch: 23/100\n",
      "108/108 [================] - 176s 2s/step - Train Loss: 0.6442 - Validation Loss: 0.6641 - Accuracy: 0.0000e+00\n",
      "Epoch: 24/100\n",
      "108/108 [================] - 184s 2s/step - Train Loss: 0.6469 - Validation Loss: 0.6674 - Accuracy: 0.0000e+00\n",
      "Epoch: 25/100\n",
      "108/108 [================] - 186s 2s/step - Train Loss: 0.6467 - Validation Loss: 0.6398 - Accuracy: 0.0000e+00\n",
      "Epoch: 26/100\n",
      "108/108 [================] - 183s 2s/step - Train Loss: 0.6513 - Validation Loss: 0.6615 - Accuracy: 0.0000e+00\n",
      "Epoch: 27/100\n",
      "108/108 [================] - 191s 2s/step - Train Loss: 0.6434 - Validation Loss: 0.6275 - Accuracy: 0.0000e+00\n",
      "Epoch: 28/100\n",
      "108/108 [================] - 180s 2s/step - Train Loss: 0.6523 - Validation Loss: 0.6518 - Accuracy: 0.0000e+00\n",
      "Epoch: 29/100\n",
      "108/108 [================] - 179s 2s/step - Train Loss: 0.6416 - Validation Loss: 0.6764 - Accuracy: 0.0000e+00\n",
      "Epoch: 30/100\n",
      "108/108 [================] - 178s 2s/step - Train Loss: 0.6433 - Validation Loss: 0.6305 - Accuracy: 0.0000e+00\n",
      "Epoch: 31/100\n",
      "108/108 [================] - 178s 2s/step - Train Loss: 0.6332 - Validation Loss: 0.6645 - Accuracy: 0.0000e+00\n",
      "Epoch: 32/100\n",
      "108/108 [================] - 188s 2s/step - Train Loss: 0.6439 - Validation Loss: 0.6700 - Accuracy: 0.0000e+00\n",
      "Epoch: 33/100\n",
      "108/108 [================] - 181s 2s/step - Train Loss: 0.6550 - Validation Loss: 0.6604 - Accuracy: 0.0000e+00\n",
      "Epoch: 34/100\n",
      "108/108 [================] - 183s 2s/step - Train Loss: 0.6457 - Validation Loss: 0.6527 - Accuracy: 0.0000e+00\n",
      "Epoch: 35/100\n",
      "108/108 [================] - 178s 2s/step - Train Loss: 0.6427 - Validation Loss: 0.6349 - Accuracy: 0.0000e+00\n",
      "Epoch: 36/100\n",
      "108/108 [================] - 186s 2s/step - Train Loss: 0.6483 - Validation Loss: 0.6737 - Accuracy: 0.0000e+00\n",
      "Epoch: 37/100\n",
      "108/108 [================] - 177s 2s/step - Train Loss: 0.6492 - Validation Loss: 0.6447 - Accuracy: 0.0000e+00\n",
      "Epoch: 38/100\n",
      "108/108 [================] - 178s 2s/step - Train Loss: 0.6356 - Validation Loss: 0.6575 - Accuracy: 0.0000e+00\n",
      "Epoch: 39/100\n",
      "108/108 [================] - 181s 2s/step - Train Loss: 0.6501 - Validation Loss: 0.6595 - Accuracy: 0.0000e+00\n",
      "Epoch: 40/100\n",
      "108/108 [================] - 179s 2s/step - Train Loss: 0.6482 - Validation Loss: 0.6396 - Accuracy: 0.0000e+00\n",
      "Epoch: 41/100\n",
      "108/108 [================] - 179s 2s/step - Train Loss: 0.6447 - Validation Loss: 0.6613 - Accuracy: 0.0000e+00\n",
      "Epoch: 42/100\n",
      "108/108 [================] - 189s 2s/step - Train Loss: 0.6378 - Validation Loss: 0.6413 - Accuracy: 0.0000e+00\n",
      "Epoch: 43/100\n",
      "108/108 [================] - 215s 2s/step - Train Loss: 0.6451 - Validation Loss: 0.6332 - Accuracy: 0.0000e+00\n",
      "Epoch: 44/100\n",
      "108/108 [================] - 176s 2s/step - Train Loss: 0.6380 - Validation Loss: 0.6147 - Accuracy: 0.0000e+00\n",
      "Epoch: 45/100\n",
      "108/108 [================] - 187s 2s/step - Train Loss: 0.6360 - Validation Loss: 0.6373 - Accuracy: 0.0000e+00\n",
      "Epoch: 46/100\n",
      "108/108 [================] - 184s 2s/step - Train Loss: 0.6445 - Validation Loss: 0.6310 - Accuracy: 0.0000e+00\n",
      "Epoch: 47/100\n",
      "108/108 [================] - 180s 2s/step - Train Loss: 0.6311 - Validation Loss: 0.6459 - Accuracy: 0.0000e+00\n",
      "Epoch: 48/100\n",
      "108/108 [================] - 178s 2s/step - Train Loss: 0.6325 - Validation Loss: 0.6582 - Accuracy: 0.0000e+00\n",
      "Epoch: 49/100\n",
      "108/108 [================] - 180s 2s/step - Train Loss: 0.6411 - Validation Loss: 0.6597 - Accuracy: 0.0000e+00\n",
      "Epoch: 50/100\n",
      "108/108 [================] - 174s 2s/step - Train Loss: 0.6333 - Validation Loss: 0.6724 - Accuracy: 0.0000e+00\n",
      "Epoch: 51/100\n",
      "108/108 [================] - 182s 2s/step - Train Loss: 0.6287 - Validation Loss: 0.6477 - Accuracy: 0.0000e+00\n",
      "Epoch: 52/100\n",
      "108/108 [================] - 175s 2s/step - Train Loss: 0.6401 - Validation Loss: 0.6601 - Accuracy: 0.0000e+00\n",
      "Epoch: 53/100\n",
      "108/108 [================] - 192s 2s/step - Train Loss: 0.6406 - Validation Loss: 0.6270 - Accuracy: 0.0000e+00\n",
      "Epoch: 54/100\n",
      "108/108 [================] - 178s 2s/step - Train Loss: 0.6402 - Validation Loss: 0.6310 - Accuracy: 0.0000e+00\n",
      "Epoch: 55/100\n",
      "108/108 [================] - 186s 2s/step - Train Loss: 0.6478 - Validation Loss: 0.6417 - Accuracy: 0.0000e+00\n",
      "Epoch: 56/100\n",
      "108/108 [================] - 192s 2s/step - Train Loss: 0.6251 - Validation Loss: 0.6555 - Accuracy: 0.0000e+00\n",
      "Epoch: 57/100\n",
      "108/108 [================] - 181s 2s/step - Train Loss: 0.6320 - Validation Loss: 0.6782 - Accuracy: 0.0000e+00\n",
      "Epoch: 58/100\n",
      "108/108 [================] - 177s 2s/step - Train Loss: 0.6355 - Validation Loss: 0.6489 - Accuracy: 0.0000e+00\n",
      "Epoch: 59/100\n",
      "108/108 [================] - 186s 2s/step - Train Loss: 0.6453 - Validation Loss: 0.6514 - Accuracy: 0.0000e+00\n",
      "Epoch: 60/100\n",
      "108/108 [================] - 183s 2s/step - Train Loss: 0.6389 - Validation Loss: 0.6396 - Accuracy: 0.0000e+00\n",
      "Epoch: 61/100\n",
      "108/108 [================] - 183s 2s/step - Train Loss: 0.6464 - Validation Loss: 0.6462 - Accuracy: 0.0000e+00\n",
      "Epoch: 62/100\n",
      "108/108 [================] - 192s 2s/step - Train Loss: 0.6315 - Validation Loss: 0.6285 - Accuracy: 0.0000e+00\n",
      "Epoch: 63/100\n",
      "108/108 [================] - 175s 2s/step - Train Loss: 0.6417 - Validation Loss: 0.6326 - Accuracy: 0.0000e+00\n",
      "Epoch: 64/100\n",
      "108/108 [================] - 176s 2s/step - Train Loss: 0.6356 - Validation Loss: 0.6502 - Accuracy: 0.0000e+00\n",
      "Epoch: 65/100\n",
      "108/108 [================] - 198s 2s/step - Train Loss: 0.6544 - Validation Loss: 0.6587 - Accuracy: 0.0000e+00\n",
      "Epoch: 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [================] - 182s 2s/step - Train Loss: 0.6303 - Validation Loss: 0.6557 - Accuracy: 0.0000e+00\n",
      "Epoch: 67/100\n",
      "108/108 [================] - 181s 2s/step - Train Loss: 0.6470 - Validation Loss: 0.6348 - Accuracy: 0.0000e+00\n",
      "Epoch: 68/100\n",
      "108/108 [================] - 173s 2s/step - Train Loss: 0.6379 - Validation Loss: 0.6445 - Accuracy: 0.0000e+00\n",
      "Epoch: 69/100\n",
      "108/108 [================] - 174s 2s/step - Train Loss: 0.6365 - Validation Loss: 0.6055 - Accuracy: 0.0000e+00\n",
      "Epoch: 70/100\n",
      "108/108 [================] - 179s 2s/step - Train Loss: 0.6383 - Validation Loss: 0.6485 - Accuracy: 0.0000e+00\n",
      "Epoch: 71/100\n",
      "108/108 [================] - 176s 2s/step - Train Loss: 0.6365 - Validation Loss: 0.6407 - Accuracy: 0.0000e+00\n",
      "Epoch: 72/100\n",
      "108/108 [================] - 186s 2s/step - Train Loss: 0.6401 - Validation Loss: 0.6485 - Accuracy: 0.0000e+00\n",
      "Epoch: 73/100\n",
      "108/108 [================] - 175s 2s/step - Train Loss: 0.6432 - Validation Loss: 0.6438 - Accuracy: 0.0000e+00\n",
      "Epoch: 74/100\n",
      "108/108 [================] - 175s 2s/step - Train Loss: 0.6328 - Validation Loss: 0.6371 - Accuracy: 0.0000e+00\n",
      "Epoch: 75/100\n",
      "108/108 [================] - 187s 2s/step - Train Loss: 0.6228 - Validation Loss: 0.6420 - Accuracy: 0.0000e+00\n",
      "Epoch: 76/100\n",
      "108/108 [================] - 194s 2s/step - Train Loss: 0.6349 - Validation Loss: 0.6192 - Accuracy: 0.0000e+00\n",
      "Epoch: 77/100\n",
      "108/108 [================] - 166s 2s/step - Train Loss: 0.6385 - Validation Loss: 0.6298 - Accuracy: 0.0000e+00\n",
      "Epoch: 78/100\n",
      "108/108 [================] - 167s 2s/step - Train Loss: 0.6280 - Validation Loss: 0.6343 - Accuracy: 0.0000e+00\n",
      "Epoch: 79/100\n",
      "108/108 [================] - 171s 2s/step - Train Loss: 0.6331 - Validation Loss: 0.6263 - Accuracy: 0.0000e+00\n",
      "Epoch: 80/100\n",
      "108/108 [================] - 172s 2s/step - Train Loss: 0.6383 - Validation Loss: 0.6382 - Accuracy: 0.0000e+00\n",
      "Epoch: 81/100\n",
      "108/108 [================] - 178s 2s/step - Train Loss: 0.6327 - Validation Loss: 0.6361 - Accuracy: 0.0000e+00\n",
      "Epoch: 82/100\n",
      "108/108 [================] - 179s 2s/step - Train Loss: 0.6512 - Validation Loss: 0.6346 - Accuracy: 0.0000e+00\n",
      "Epoch: 83/100\n",
      "108/108 [================] - 175s 2s/step - Train Loss: 0.6356 - Validation Loss: 0.6431 - Accuracy: 0.0000e+00\n",
      "Epoch: 84/100\n",
      "108/108 [================] - 178s 2s/step - Train Loss: 0.6339 - Validation Loss: 0.6699 - Accuracy: 0.0000e+00\n",
      "Epoch: 85/100\n",
      "108/108 [================] - 183s 2s/step - Train Loss: 0.6315 - Validation Loss: 0.6449 - Accuracy: 0.0000e+00\n",
      "Epoch: 86/100\n",
      "108/108 [================] - 204s 2s/step - Train Loss: 0.6365 - Validation Loss: 0.6889 - Accuracy: 0.0000e+00\n",
      "Epoch: 87/100\n",
      "108/108 [================] - 170s 2s/step - Train Loss: 0.6259 - Validation Loss: 0.6293 - Accuracy: 0.0000e+00\n",
      "Epoch: 88/100\n",
      "108/108 [================] - 170s 2s/step - Train Loss: 0.6385 - Validation Loss: 0.6344 - Accuracy: 0.0000e+00\n",
      "Epoch: 89/100\n",
      "108/108 [================] - 179s 2s/step - Train Loss: 0.6391 - Validation Loss: 0.6376 - Accuracy: 0.0000e+00\n",
      "Epoch: 90/100\n",
      "108/108 [================] - 169s 2s/step - Train Loss: 0.6371 - Validation Loss: 0.6697 - Accuracy: 0.0000e+00\n",
      "Epoch: 91/100\n",
      "108/108 [================] - 184s 2s/step - Train Loss: 0.6290 - Validation Loss: 0.6290 - Accuracy: 0.0000e+00\n",
      "Epoch: 92/100\n",
      "108/108 [================] - 180s 2s/step - Train Loss: 0.6269 - Validation Loss: 0.6322 - Accuracy: 0.0000e+00\n",
      "Epoch: 93/100\n",
      "108/108 [================] - 184s 2s/step - Train Loss: 0.6290 - Validation Loss: 0.6375 - Accuracy: 0.0000e+00\n",
      "Epoch: 94/100\n",
      "108/108 [================] - 171s 2s/step - Train Loss: 0.6275 - Validation Loss: 0.6489 - Accuracy: 0.0000e+00\n",
      "Epoch: 95/100\n",
      "108/108 [================] - 172s 2s/step - Train Loss: 0.6304 - Validation Loss: 0.6567 - Accuracy: 0.0000e+00\n",
      "Epoch: 96/100\n",
      "108/108 [================] - 176s 2s/step - Train Loss: 0.6480 - Validation Loss: 0.6352 - Accuracy: 0.0000e+00\n",
      "Epoch: 97/100\n",
      "108/108 [================] - 174s 2s/step - Train Loss: 0.6380 - Validation Loss: 0.6379 - Accuracy: 0.0000e+00\n",
      "Epoch: 98/100\n",
      "108/108 [================] - 167s 2s/step - Train Loss: 0.6219 - Validation Loss: 0.6421 - Accuracy: 0.0000e+00\n",
      "Epoch: 99/100\n",
      "108/108 [================] - 181s 2s/step - Train Loss: 0.6505 - Validation Loss: 0.6358 - Accuracy: 0.0000e+00\n",
      "Epoch: 100/100\n",
      "108/108 [================] - 177s 2s/step - Train Loss: 0.6177 - Validation Loss: 0.6476 - Accuracy: 0.0000e+00\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = trainer.train_and_evaluate(100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86865199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/checkpoint_detr_hd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd 1/4:\n",
    "    Train Loss: 0.6584 - Validation Loss: 0.6669 - Accuracy: 0.0000e+00\n",
    "    best: Train Loss: 0.6584 - Validation Loss: 0.6498 - Accuracy: 0.0000e+00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4929e4f8",
   "metadata": {},
   "source": [
    "Train Loss: 0.6197 - Validation Loss: 0.6037 - Accuracy: 0.0000e+00\n",
    "best: Train Loss: 0.6129 - Validation Loss: 0.5986 - Accuracy: 0.0000e+00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dcfad",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd143b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = valid_dset[0]\n",
    "image = a[0].unsqueeze(0).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f12fd480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/models/position_encoding.py:84: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    }
   ],
   "source": [
    "b = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1880a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([1, 2, 3, 4, 5, 6, 7]),\n",
       " 'boxes': tensor([[0.1058, 0.0865, 0.0288, 0.2115, 0.6429, 0.2143],\n",
       "         [0.0769, 0.0962, 0.0385, 0.1538, 0.7143, 0.2857],\n",
       "         [0.2212, 0.1154, 0.0673, 0.4423, 0.8571, 0.5000],\n",
       "         [0.0962, 0.1442, 0.0865, 0.1923, 1.0714, 0.6429],\n",
       "         [0.1731, 0.1731, 0.0673, 0.3462, 1.2857, 0.5000],\n",
       "         [0.0769, 0.1442, 0.0962, 0.1538, 1.0714, 0.7143],\n",
       "         [0.1731, 0.0769, 0.0769, 0.3462, 0.5714, 0.5714]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d582811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 2, 2, 3, 2, 3, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['pred_logits'].argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b79ac25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2162, 0.1905, 0.1379, 0.4125, 0.7468, 0.5607],\n",
       "         [0.1852, 0.1596, 0.1057, 0.3731, 0.6936, 0.5030],\n",
       "         [0.2007, 0.1832, 0.1221, 0.3869, 0.7135, 0.5342],\n",
       "         [0.1799, 0.1373, 0.0945, 0.3724, 0.6251, 0.4239],\n",
       "         [0.2030, 0.1737, 0.1308, 0.4168, 0.7406, 0.5684],\n",
       "         [0.1633, 0.1215, 0.0784, 0.3160, 0.4827, 0.3158],\n",
       "         [0.2196, 0.1901, 0.1439, 0.4387, 0.7995, 0.6313],\n",
       "         [0.2001, 0.1834, 0.1260, 0.4072, 0.7581, 0.5545]]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['pred_boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97e3bd",
   "metadata": {},
   "source": [
    "# Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "362d1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_thing_map = {'0': False, '1': True, '2': True, '3': True, '4': True, '5': True, '6': True, '7': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4676c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process_panoptic = PostProcessPanoptic(is_thing_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5321fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_masks = outputs['pred_masks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b528de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = post_process_panoptic(outputs,[tuple(torch.tensor(image.squeeze().shape).tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "560c90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_segmented_image(aux_target['seg_im'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e32c933d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 25])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_inpt[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
