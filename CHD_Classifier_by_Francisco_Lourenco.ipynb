{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR, LossCustom\n",
    "from models.segmentation import DETRsegm, PostProcessPanoptic\n",
    "from models.matcher import HungarianMatcher\n",
    "from hubconf import detr_resnet101_panoptic, detr_resnet3d_panoptic\n",
    "from torchvision.transforms import Resize\n",
    "from util.misc import get_world_size\n",
    "import open3d as o3d\n",
    "import torch.optim as optim\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e667078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.imageglobals.logger.level = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, target):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(target)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ca9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_downsample_image(image, down_scale = 8):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], down_scale)\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], down_scale)\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], down_scale)\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b1ff6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_image_to_given_size(image, size = 128):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], int(image_shape[0]/size))\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], int(image_shape[1]/size))\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], int(image_shape[2]/size))\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e187e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmented_image(segmented_image):\n",
    "#     segmented_image.squeeze()\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    colors = [[128,128,128],[255,0,0],[255,255,0],[0,255,0],[0,255,255],[0,0,255],[255,0,255]]\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    for _class in classes[1:]:\n",
    "        points_numpy = (segmented_image == _class).nonzero().numpy()\n",
    "        o3d_point_cloud = o3d.geometry.PointCloud()\n",
    "        o3d_point_cloud.points = o3d.utility.Vector3dVector(points_numpy)\n",
    "        o3d_point_cloud.estimate_normals()\n",
    "        o3d_point_cloud.paint_uniform_color(np.array(colors[int(_class)-1])/255)\n",
    "        vis.add_geometry(o3d_point_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c45f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    bb_list = []\n",
    "    for class_ in classes[1:]: \n",
    "        points = (segmented_image == class_).nonzero()\n",
    "        x_min, x_max = points[:,0].min(), points[:,0].max()\n",
    "        y_min, y_max = points[:,1].min(), points[:,1].max()\n",
    "        z_min, z_max = points[:,2].min(), points[:,2].max()\n",
    "        bb = torch.tensor([(x_max-x_min)/2, (y_max-y_min)/2, (z_max-z_min)/2, x_max-x_min, y_max-y_min, z_max-z_min])\n",
    "        bb[0::2] = bb[0::2]/torch.tensor(segmented_image.shape)\n",
    "        bb[1::2] = bb[1::2]/torch.tensor(segmented_image.shape)\n",
    "        bb_list.append(bb)\n",
    "    bbs = torch.stack(bb_list)\n",
    "    return bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc5678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    masks_list = []\n",
    "    for class_ in classes[1:]:\n",
    "        mask = segmented_image.clone()\n",
    "        mask[segmented_image == class_] = 1\n",
    "        mask[segmented_image != class_] = 0\n",
    "        masks_list.append(mask.short())\n",
    "    masks = torch.stack(masks_list)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(segmented_image):\n",
    "    return segmented_image.unique().long()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8630212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_dict_panoptic(segmented_image):\n",
    "    labels = get_labels(segmented_image)\n",
    "    boxes = get_bounding_boxes(segmented_image)\n",
    "    masks = get_masks(segmented_image)\n",
    "    target_dict = {'labels': labels, 'boxes': boxes, 'masks': masks, 'seg_im': segmented_image}\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b3d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_dict_detection(segmented_image):\n",
    "    labels = get_labels(segmented_image)\n",
    "    boxes = get_bounding_boxes(segmented_image)\n",
    "    target_dict = {'labels': labels, 'boxes': boxes}\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d489865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_corners(box):\n",
    "    x_min = box[0] - box[3]/2\n",
    "    x_max = box[0] + box[3]/2\n",
    "    y_min = box[1] - box[4]/2\n",
    "    y_max = box[1] + box[4]/2\n",
    "    z_min = box[2] - box[5]/2\n",
    "    z_max = box[2] + box[5]/2\n",
    "    return torch.tensor([x_min,y_min,z_min,x_max,y_max,z_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a57447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersetion_box_corners(box_1,box_2):\n",
    "    intersection_box_corners = torch.zeros(6)\n",
    "    intersection_box_corners[0] = max(box_1[0],box_2[0])\n",
    "    intersection_box_corners[1] = min(box_1[3],box_2[3])\n",
    "    intersection_box_corners[2] = max(box_1[1],box_2[1])\n",
    "    intersection_box_corners[3] = min(box_1[4],box_2[4])\n",
    "    intersection_box_corners[4] = max(box_1[2],box_2[2])\n",
    "    intersection_box_corners[5] = min(box_1[5],box_2[5])\n",
    "    return intersection_box_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74035a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_box_volume(box):\n",
    "    box_volume = (box[3]-box[0])*(box[4]-box[1])*(box[5]-box[2])\n",
    "    if box_volume < 0: box_volume = 0\n",
    "    return box_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c704946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_box_iou(box_1,box_2):\n",
    "    box_1_corners = get_box_corners(box_1)\n",
    "    box_2_corners = get_box_corners(box_2)\n",
    "    intersection_box = get_intersetion_box_corners(box_1_corners,box_2_corners)\n",
    "    intersection_box_volume = compute_box_volume(intersection_box)\n",
    "    if intersection_box_volume == 0: return 0\n",
    "    box_1_volume = compute_box_volume(box_1_corners) \n",
    "    box_2_volume = compute_box_volume(box_2_corners)\n",
    "    iou = intersection_box_volume/(box_1_volume+box_2_volume-intersection_box_volume)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1eed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_precision(outputs, target, labels):\n",
    "    ious = []\n",
    "    ap = []\n",
    "    for i, label in enumerate(labels):\n",
    "        pred_box = outputs['pred_boxes'].squeeze()[i]\n",
    "        target_box = target['boxes'][label] \n",
    "        iou = compute_box_iou(pred_box,target_box)\n",
    "        ious.append(iou)\n",
    "    ious_tensor = torch.tensor(ious)\n",
    "    for t in range(50,95,5):\n",
    "        tp = (ious_tensor >= t/100).count_nonzero()\n",
    "        fn = abs(len(target['labels']) - 1 - tp)\n",
    "        fp = abs((outputs['pred_logits'].squeeze().argmax(1) != 0).count_nonzero() - 1 - tp)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        ap.append(precision*recall)\n",
    "    ap_tensor = torch.tensor(ap)\n",
    "    return ap_tensor.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa552c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_panoptic_quality(pred_seg, target):\n",
    "    ious = []\n",
    "    for i, label in enumerate(target['labels'][1:]):\n",
    "        target_mask = target['masks'][i] \n",
    "        pred_mask = (pred_seg == label).int()\n",
    "        pred_mask[pred_mask == 0] = -1\n",
    "        intersection = (pred_mask == target_mask).count_nonzero()\n",
    "        union = pred_mask[pred_mask == 1].count_nonzero() + target_mask[target_mask == 1].count_nonzero() - intersection\n",
    "        iou = intersection/union\n",
    "        ious.append(iou)\n",
    "    ious_tensor = torch.tensor(ious)\n",
    "    tp = (ious_tensor > 0.5).count_nonzero()\n",
    "    fn = abs(len(target['labels']) - 1 - tp)\n",
    "    fp = abs(len(pred_seg.unique()) - 1 - tp)\n",
    "    if tp > 0:\n",
    "        sq = ious_tensor[ious_tensor > 0.5].sum() / tp\n",
    "        rq = rq = tp / (tp + fp/2 + fn/2)\n",
    "        pq = sq*rq\n",
    "    else:\n",
    "        sq = 0\n",
    "        rq = 0\n",
    "        pq = 0\n",
    "    return pq, sq, rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cbf87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForDetection(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, target_paths, down_scale=8):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.down_scale = down_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(self.image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array).float()\n",
    "        downsampled_image = uniform_downsample_image(image_torch_tensor, down_scale=self.down_scale)\n",
    "        downsampled_image = downsampled_image/downsampled_image.max()\n",
    "#         downsampled_image = downsample_image_to_given_size(image_torch_tensor, size=64)\n",
    "        target_np_array = nib.load(self.target_paths[i]).get_fdata()\n",
    "        target_torch_tensor = torch.from_numpy(target_np_array)\n",
    "        target_torch_tensor[target_torch_tensor > 7] = 0\n",
    "        downsampled_target = uniform_downsample_image(target_torch_tensor, down_scale=self.down_scale)\n",
    "#         downsampled_target = downsample_image_to_given_size(target_torch_tensor, size=64)\n",
    "        target_dict = create_target_dict_detection(downsampled_target)\n",
    "        return downsampled_image, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a11d132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, target_paths, down_scale=8):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.down_scale = down_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(self.image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array).float()\n",
    "        downsampled_image = uniform_downsample_image(image_torch_tensor, down_scale=self.down_scale)\n",
    "#         downsampled_image = downsampled_image/downsampled_image.max()\n",
    "#         downsampled_image = downsample_image_to_given_size(image_torch_tensor, size=64)\n",
    "        target_np_array = nib.load(self.target_paths[i]).get_fdata()\n",
    "        target_torch_tensor = torch.from_numpy(target_np_array)\n",
    "        target_torch_tensor[target_torch_tensor > 7] = 0\n",
    "        downsampled_target = uniform_downsample_image(target_torch_tensor, down_scale=self.down_scale)\n",
    "#         downsampled_target = downsample_image_to_given_size(target_torch_tensor, size=64)\n",
    "        target_dict = create_target_dict_panoptic(downsampled_target)\n",
    "        return downsampled_image, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6625d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dset_path, dset_type, split_index=87, down_scale=10):\n",
    "    image_paths = glob(f'{dset_path}/*image.nii.gz',recursive=True)\n",
    "    target_paths = glob(f'{dset_path}/*label.nii.gz',recursive=True)\n",
    "    image_paths.sort(key=sort_func)\n",
    "    target_paths.sort(key=sort_func)\n",
    "    if dset_type == 'detection':\n",
    "        train_dset = DatasetForDetection(image_paths[:split_index], target_paths[:split_index], down_scale=down_scale)\n",
    "        valid_dset = DatasetForDetection(image_paths[split_index:],target_paths[split_index:], down_scale=down_scale)\n",
    "    if dset_type == 'panoptic':\n",
    "        train_dset = DatasetForSegmentation(image_paths[:split_index], target_paths[:split_index], down_scale=down_scale)\n",
    "        valid_dset = DatasetForSegmentation(image_paths[split_index:], target_paths[split_index:], down_scale=down_scale)\n",
    "    return train_dset, valid_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16883bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndEvaluate():\n",
    "    def __init__(self, model, optimizer, loss, train_dset, valid_dset):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.train_dset = train_dset\n",
    "        self.valid_dset = valid_dset\n",
    "        self.matcher = loss.matcher \n",
    "        aux_target = train_dset[0][1]\n",
    "        if 'masks' in aux_target.keys():\n",
    "            self.masks = True\n",
    "            self.key = 'loss_mask'\n",
    "            is_thing_map = {'0': False, '1': True, '2': True, '3': True, '4': True, '5': True, '6': True, '7': True}\n",
    "            self.post_process_panoptic = PostProcessPanoptic(is_thing_map)\n",
    "        else: \n",
    "            self.masks = False\n",
    "            self.key = 'loss_bbox'        \n",
    "    \n",
    "    def train_and_evaluate(self, n_epoch, save=False):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        for epoch in range(n_epoch):\n",
    "            kbar = pkbar.Kbar(target=(len(self.train_dset)+len(self.valid_dset)-2), epoch=epoch, num_epochs=n_epoch, width=16)\n",
    "            running_loss_t = 0.0\n",
    "            running_loss_v = 0.0\n",
    "            rdm = torch.randperm(len(self.train_dset))\n",
    "            \n",
    "            for i, j in enumerate(rdm):\n",
    "                b = self.train_dset[j]\n",
    "                image = b[0]\n",
    "                image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "                target = b[1]\n",
    "                for t in target.items():\n",
    "                    target[t[0]] = t[1].to(device)\n",
    "                outputs = self.model(image)\n",
    "                self.optimizer.zero_grad()\n",
    "                if self.masks:\n",
    "                    l = self.loss.loss_masks(outputs, [target]) #To train masks\n",
    "                else:\n",
    "                    l = self.loss.loss_boxes(outputs, [target]) #To train boxes\n",
    "                l[self.key].backward() \n",
    "                self.optimizer.step()\n",
    "                running_loss_t = l[self.key].item()\n",
    "                kbar.update(i, values=[(\"Train Loss\", running_loss_t)])\n",
    "            \n",
    "            if save:\n",
    "                torch.save(self.model.state_dict(), '/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/checkpoint_detr.pth')\n",
    "            \n",
    "            kbar.add(1, values=[(\"Validation Loss\", 0), (\"Accuracy\", 0)])        \n",
    "            with torch.no_grad():\n",
    "                for ii, b in enumerate(self.valid_dset):\n",
    "                    image = b[0]\n",
    "                    image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "                    target = b[1]\n",
    "                    for t in target.items():\n",
    "                        target[t[0]] = t[1].to(device)\n",
    "\n",
    "                    outputs = self.model(image)\n",
    "                    if self.masks:\n",
    "                        l = self.loss.loss_masks(outputs, [target]) #To train masks\n",
    "                    else:\n",
    "                        l = self.loss.loss_boxes(outputs, [target]) #To train boxes\n",
    "                    running_loss_v = l[self.key].item()\n",
    "                    src_idx, trgt_idx = self.matcher(outputs,[target])[0]\n",
    "                    if self.masks:\n",
    "                        pred_seg = post_process_panoptic(outputs[trgt_idx],[tuple(torch.tensor(b[0].shape).tolist())]).to(device)\n",
    "                        pq,sq,rq = compute_panoptic_quality(pred_seg, target)\n",
    "                        kbar.update(i+ii, values=[(\"Validation Loss\", running_loss_v), (\"PQ\", 100*pq),(\"SQ\", 100*sq),(\"RQ\", 100*rq)])\n",
    "                    else:\n",
    "                        ap = compute_average_precision(outputs, target, trgt_idx) #For boxes\n",
    "                        kbar.update(i+ii, values=[(\"Validation Loss\", running_loss_v), (\"Accuracy\", 100*ap)]) #For boxes\n",
    "        print('Finished Training')\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = '/home/francisco/workspace/ImageCHD_dataset'\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da2874c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = detr_resnet3d_panoptic()\n",
    "model = model.detr #To train boxes\n",
    "# model.load_state_dict(torch.load('/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/checkpoint_detr.pth'))\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f5ba723",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher()\n",
    "loss = LossCustom(matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae78ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset, valid_dset = create_dataset(path, 'detection', 87, 10)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=10e-4)\n",
    "trainer = TrainAndEvaluate(model, optimizer, loss, train_dset, valid_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0efa0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25\n",
      "108/108 [================] - 157s 1s/step - Train Loss: 0.6554 - Validation Loss: 0.6580 - Accuracy: 0.0000e+00\n",
      "Epoch: 2/25\n",
      "108/108 [================] - 160s 1s/step - Train Loss: 0.6545 - Validation Loss: 0.6667 - Accuracy: 0.0000e+00\n",
      "Epoch: 3/25\n",
      "108/108 [================] - 151s 1s/step - Train Loss: 0.6556 - Validation Loss: 0.6749 - Accuracy: 0.0000e+00\n",
      "Epoch: 4/25\n",
      "108/108 [================] - 163s 2s/step - Train Loss: 0.6518 - Validation Loss: 0.6467 - Accuracy: 0.0000e+00\n",
      "Epoch: 5/25\n",
      "108/108 [================] - 151s 1s/step - Train Loss: 0.6743 - Validation Loss: 0.6550 - Accuracy: 0.0000e+00\n",
      "Epoch: 6/25\n",
      "108/108 [================] - 155s 1s/step - Train Loss: 0.6453 - Validation Loss: 0.6421 - Accuracy: 0.0000e+00\n",
      "Epoch: 7/25\n",
      "108/108 [================] - 156s 1s/step - Train Loss: 0.6599 - Validation Loss: 0.6271 - Accuracy: 0.0000e+00\n",
      "Epoch: 8/25\n",
      "108/108 [================] - 148s 1s/step - Train Loss: 0.6358 - Validation Loss: 0.6380 - Accuracy: 0.0000e+00\n",
      "Epoch: 9/25\n",
      "108/108 [================] - 149s 1s/step - Train Loss: 0.6469 - Validation Loss: 0.6425 - Accuracy: 0.0000e+00\n",
      "Epoch: 10/25\n",
      "108/108 [================] - 157s 1s/step - Train Loss: 0.6681 - Validation Loss: 0.6471 - Accuracy: 0.0000e+00\n",
      "Epoch: 11/25\n",
      "108/108 [================] - 166s 2s/step - Train Loss: 0.6494 - Validation Loss: 0.6350 - Accuracy: 0.0000e+00\n",
      "Epoch: 12/25\n",
      "108/108 [================] - 173s 2s/step - Train Loss: 0.6468 - Validation Loss: 0.6435 - Accuracy: 0.0000e+00\n",
      "Epoch: 13/25\n",
      "108/108 [================] - 171s 2s/step - Train Loss: 0.6476 - Validation Loss: 0.6260 - Accuracy: 0.0000e+00\n",
      "Epoch: 14/25\n",
      " 59/108 [=======>........] - ETA: 1:12 - Train Loss: 0.6485"
     ]
    }
   ],
   "source": [
    "model = trainer.train_and_evaluate(25, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86865199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/checkpoint_detr.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189120f5",
   "metadata": {},
   "source": [
    "Train Loss: 0.6589 - Validation Loss: 0.6519 - Accuracy: nan\n",
    "best: Train Loss: 0.6478 - Validation Loss: 0.6197 - Accuracy: nan\n",
    "\n",
    "Train Loss: 0.6589 - Validation Loss: 0.6519 - Accuracy: 0.0000e+00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dcfad",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd143b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = valid_dset[0]\n",
    "image = a[0].unsqueeze(0).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f12fd480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/models/position_encoding.py:84: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    }
   ],
   "source": [
    "b = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1880a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([1, 2, 3, 4, 5, 6, 7]),\n",
       " 'boxes': tensor([[0.1058, 0.0865, 0.0288, 0.2115, 0.6429, 0.2143],\n",
       "         [0.0769, 0.0962, 0.0385, 0.1538, 0.7143, 0.2857],\n",
       "         [0.2212, 0.1154, 0.0673, 0.4423, 0.8571, 0.5000],\n",
       "         [0.0962, 0.1442, 0.0865, 0.1923, 1.0714, 0.6429],\n",
       "         [0.1731, 0.1731, 0.0673, 0.3462, 1.2857, 0.5000],\n",
       "         [0.0769, 0.1442, 0.0962, 0.1538, 1.0714, 0.7143],\n",
       "         [0.1731, 0.0769, 0.0769, 0.3462, 0.5714, 0.5714]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d582811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 2, 2, 3, 2, 3, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['pred_logits'].argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b79ac25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2162, 0.1905, 0.1379, 0.4125, 0.7468, 0.5607],\n",
       "         [0.1852, 0.1596, 0.1057, 0.3731, 0.6936, 0.5030],\n",
       "         [0.2007, 0.1832, 0.1221, 0.3869, 0.7135, 0.5342],\n",
       "         [0.1799, 0.1373, 0.0945, 0.3724, 0.6251, 0.4239],\n",
       "         [0.2030, 0.1737, 0.1308, 0.4168, 0.7406, 0.5684],\n",
       "         [0.1633, 0.1215, 0.0784, 0.3160, 0.4827, 0.3158],\n",
       "         [0.2196, 0.1901, 0.1439, 0.4387, 0.7995, 0.6313],\n",
       "         [0.2001, 0.1834, 0.1260, 0.4072, 0.7581, 0.5545]]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['pred_boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97e3bd",
   "metadata": {},
   "source": [
    "# Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "362d1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_thing_map = {'0': False, '1': True, '2': True, '3': True, '4': True, '5': True, '6': True, '7': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4676c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process_panoptic = PostProcessPanoptic(is_thing_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5321fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_masks = outputs['pred_masks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b528de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = post_process_panoptic(outputs,[tuple(torch.tensor(image.squeeze().shape).tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "560c90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_segmented_image(aux_target['seg_im'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e32c933d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 25])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_inpt[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
