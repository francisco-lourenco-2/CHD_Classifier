{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR\n",
    "from models.segmentation import DETRsegm\n",
    "from models.matcher import HungarianMatcher\n",
    "from hubconf import detr_resnet101_panoptic, detr_resnet3d_panoptic\n",
    "from torchvision.transforms import Resize\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, label):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(label)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ca9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_downsample_image(image, down_scale = 8):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], down_scale)\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], down_scale)\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], down_scale)\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd9f6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, label_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array)\n",
    "        label_np_array = nib.load(label_paths[i]).get_fdata()\n",
    "        label_torch_tensor = torch.from_numpy(label_np_array)\n",
    "        return image_torch_tensor, label_torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e187e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmented_image(segmented_image):\n",
    "#     segmented_image.squeeze()\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    colors = [[128,128,128],[255,0,0],[255,255,0],[0,255,0],[0,255,255],[0,0,255],[255,0,255]]\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    for _class in classes[1:]:\n",
    "        points_numpy = (segmented_image == _class).nonzero().numpy()\n",
    "        o3d_point_cloud = o3d.geometry.PointCloud()\n",
    "        o3d_point_cloud.points = o3d.utility.Vector3dVector(points_numpy)\n",
    "        o3d_point_cloud.estimate_normals()\n",
    "        o3d_point_cloud.paint_uniform_color(np.array(colors[int(_class)-1])/255)\n",
    "        vis.add_geometry(o3d_point_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c45f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    bb_list = []\n",
    "    for class_ in classes: \n",
    "        points = (segmented_image == 1).nonzero()\n",
    "        x_min, x_max = points[:,0].min(), points[:,0].max()\n",
    "        y_min, y_max = points[:,1].min(), points[:,1].max()\n",
    "        z_min, z_max = points[:,2].min(), points[:,2].max()\n",
    "        bb = torch.tensor([(x_max-x_min)/2, (y_max-y_min)/2, (z_max-z_min)/2, x_max-x_min, y_max-y_min, z_max-z_min])\n",
    "        bb[0::2] = bb[0::2]/torch.tensor(segmented_image.shape)\n",
    "        bb[1::2] = bb[1::2]/torch.tensor(segmented_image.shape)\n",
    "        bb_list.append(bb)\n",
    "    return bb_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "864b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/francisco/workspace/ImageCHD_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd529519",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob(f'{path}/*image.nii.gz',recursive=True)\n",
    "label_paths = glob(f'{path}/*label.nii.gz',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b202a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths.sort(key=sort_func)\n",
    "label_paths.sort(key=sort_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = DatasetForSegmentation(image_paths,label_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f118759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-17 19:00:29,571 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-17 19:00:30,147 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    }
   ],
   "source": [
    "image, label = dset[2]\n",
    "image = image.to(device)\n",
    "label = label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "914158a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_image = uniform_downsample_image(image)\n",
    "downsampled_label = uniform_downsample_image(label)\n",
    "downsampled_image_reshaped = downsampled_image.unsqueeze(0).unsqueeze(0).float()\n",
    "downsampled_label_reshaped = downsampled_label.unsqueeze(0).unsqueeze(0)\n",
    "# im_right_shape = im.transpose(2,0).unsqueeze(0).unsqueeze(0).float()\n",
    "# label_right_shape = label.transpose(2,0).unsqueeze(0).unsqueeze(0).float()\n",
    "# new_size = [int(im_right_shape.shape[2]/8),int(im_right_shape.shape[3]/8),int(im_right_shape.shape[4]/8)]\n",
    "# im_resized = im_right_shape.resize_((1,1,new_size[0],new_size[1],new_size[2]))\n",
    "# label_resized = label_right_shape.resize_((1,1,new_size[0],new_size[1],new_size[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ff5b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "classes = segmented_image.unique()\n",
    "masks_list = []\n",
    "for class_ in classes[1:]:\n",
    "    mask = segmented_image.clone()\n",
    "    mask[segmented_image == class_] = 1\n",
    "    mask[segmented_image != class_] = 0\n",
    "    masks_list.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0931239d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f92b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = torch.tensor([(x_max-x_min)/2, (y_max-y_min)/2, (z_max-z_min)/2, x_max-x_min, y_max-y_min, z_max-z_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a12b9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb[0::2] = bb[0::2]/torch.tensor(segmented_image.shape)\n",
    "bb[1::2] = bb[1::2]/torch.tensor(segmented_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2aaff244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0859, 0.1016, 0.0625, 0.1719, 0.5000, 0.3077])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcd84697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.5000,  6.5000,  4.0000, 11.0000, 13.0000,  8.0000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb/torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4661092",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to cast Python instance to C++ type (compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualize_segmented_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownsampled_label_reshaped\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mvisualize_segmented_image\u001b[0;34m(segmented_image)\u001b[0m\n\u001b[1;32m      9\u001b[0m points_numpy \u001b[38;5;241m=\u001b[39m (segmented_image \u001b[38;5;241m==\u001b[39m _class)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     10\u001b[0m o3d_point_cloud \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mPointCloud()\n\u001b[0;32m---> 11\u001b[0m o3d_point_cloud\u001b[38;5;241m.\u001b[39mpoints \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutility\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVector3dVector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m o3d_point_cloud\u001b[38;5;241m.\u001b[39mestimate_normals()\n\u001b[1;32m     13\u001b[0m o3d_point_cloud\u001b[38;5;241m.\u001b[39mpaint_uniform_color(np\u001b[38;5;241m.\u001b[39marray(colors[\u001b[38;5;28mint\u001b[39m(_class)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to cast Python instance to C++ type (compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "visualize_segmented_image(downsampled_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da2874c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detr_seg = detr_resnet3d_panoptic()\n",
    "# detr_seg = detr_resnet11_panoptic()\n",
    "detr_seg.eval();\n",
    "detr_seg.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3190fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = torch.ones((1, 1, 32, 64, 64),device=device)\n",
    "# im = torch.ones((1, 3, 128, 128),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9c0ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/models/position_encoding.py:84: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    }
   ],
   "source": [
    "outputs = detr_seg(downsampled_image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783f6edf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4847, 0.4700, 0.5368, 0.4979, 0.4643, 0.4722],\n",
       "         [0.4852, 0.4694, 0.5366, 0.4982, 0.4648, 0.4723],\n",
       "         [0.4852, 0.4695, 0.5366, 0.4985, 0.4648, 0.4723],\n",
       "         [0.4852, 0.4697, 0.5366, 0.4987, 0.4645, 0.4724],\n",
       "         [0.4852, 0.4692, 0.5365, 0.4985, 0.4651, 0.4724],\n",
       "         [0.4853, 0.4695, 0.5367, 0.4983, 0.4645, 0.4724],\n",
       "         [0.4848, 0.4698, 0.5367, 0.4982, 0.4640, 0.4722],\n",
       "         [0.4852, 0.4692, 0.5366, 0.4985, 0.4642, 0.4722],\n",
       "         [0.4849, 0.4696, 0.5368, 0.4979, 0.4641, 0.4725],\n",
       "         [0.4849, 0.4694, 0.5367, 0.4979, 0.4639, 0.4720],\n",
       "         [0.4849, 0.4694, 0.5368, 0.4981, 0.4642, 0.4722],\n",
       "         [0.4854, 0.4696, 0.5365, 0.4987, 0.4646, 0.4722],\n",
       "         [0.4850, 0.4696, 0.5369, 0.4981, 0.4645, 0.4722],\n",
       "         [0.4852, 0.4692, 0.5364, 0.4985, 0.4642, 0.4723],\n",
       "         [0.4849, 0.4695, 0.5367, 0.4980, 0.4639, 0.4723],\n",
       "         [0.4852, 0.4699, 0.5365, 0.4985, 0.4647, 0.4727],\n",
       "         [0.4852, 0.4692, 0.5367, 0.4981, 0.4645, 0.4720],\n",
       "         [0.4854, 0.4693, 0.5365, 0.4983, 0.4648, 0.4721],\n",
       "         [0.4850, 0.4694, 0.5367, 0.4983, 0.4642, 0.4723],\n",
       "         [0.4851, 0.4697, 0.5366, 0.4987, 0.4642, 0.4722],\n",
       "         [0.4847, 0.4700, 0.5367, 0.4986, 0.4647, 0.4726],\n",
       "         [0.4851, 0.4699, 0.5367, 0.4986, 0.4641, 0.4725],\n",
       "         [0.4852, 0.4695, 0.5368, 0.4980, 0.4638, 0.4723],\n",
       "         [0.4848, 0.4698, 0.5369, 0.4983, 0.4641, 0.4721],\n",
       "         [0.4851, 0.4697, 0.5366, 0.4988, 0.4648, 0.4722],\n",
       "         [0.4849, 0.4697, 0.5365, 0.4981, 0.4646, 0.4723],\n",
       "         [0.4852, 0.4701, 0.5365, 0.4986, 0.4649, 0.4724],\n",
       "         [0.4849, 0.4697, 0.5370, 0.4982, 0.4642, 0.4725],\n",
       "         [0.4847, 0.4695, 0.5368, 0.4980, 0.4644, 0.4723],\n",
       "         [0.4851, 0.4696, 0.5367, 0.4977, 0.4645, 0.4722],\n",
       "         [0.4850, 0.4691, 0.5368, 0.4982, 0.4641, 0.4726],\n",
       "         [0.4849, 0.4699, 0.5369, 0.4982, 0.4637, 0.4723],\n",
       "         [0.4849, 0.4699, 0.5368, 0.4982, 0.4641, 0.4723],\n",
       "         [0.4851, 0.4694, 0.5367, 0.4984, 0.4641, 0.4721],\n",
       "         [0.4854, 0.4695, 0.5366, 0.4987, 0.4645, 0.4722],\n",
       "         [0.4852, 0.4696, 0.5366, 0.4986, 0.4644, 0.4725],\n",
       "         [0.4851, 0.4693, 0.5367, 0.4979, 0.4643, 0.4719],\n",
       "         [0.4847, 0.4700, 0.5367, 0.4984, 0.4646, 0.4721],\n",
       "         [0.4854, 0.4693, 0.5365, 0.4986, 0.4645, 0.4724],\n",
       "         [0.4848, 0.4694, 0.5367, 0.4981, 0.4647, 0.4722],\n",
       "         [0.4852, 0.4693, 0.5364, 0.4984, 0.4644, 0.4721],\n",
       "         [0.4851, 0.4697, 0.5369, 0.4984, 0.4647, 0.4724],\n",
       "         [0.4850, 0.4694, 0.5366, 0.4984, 0.4647, 0.4722],\n",
       "         [0.4850, 0.4694, 0.5366, 0.4982, 0.4643, 0.4721],\n",
       "         [0.4845, 0.4693, 0.5367, 0.4983, 0.4646, 0.4723],\n",
       "         [0.4852, 0.4699, 0.5364, 0.4985, 0.4650, 0.4722],\n",
       "         [0.4850, 0.4692, 0.5368, 0.4981, 0.4643, 0.4725],\n",
       "         [0.4855, 0.4701, 0.5369, 0.4982, 0.4640, 0.4724],\n",
       "         [0.4852, 0.4693, 0.5367, 0.4985, 0.4645, 0.4723],\n",
       "         [0.4852, 0.4699, 0.5366, 0.4985, 0.4645, 0.4719],\n",
       "         [0.4850, 0.4694, 0.5367, 0.4980, 0.4641, 0.4723],\n",
       "         [0.4850, 0.4698, 0.5366, 0.4980, 0.4646, 0.4723],\n",
       "         [0.4848, 0.4696, 0.5367, 0.4983, 0.4646, 0.4722],\n",
       "         [0.4845, 0.4699, 0.5368, 0.4979, 0.4641, 0.4721],\n",
       "         [0.4851, 0.4695, 0.5366, 0.4988, 0.4646, 0.4721],\n",
       "         [0.4849, 0.4697, 0.5368, 0.4983, 0.4648, 0.4721],\n",
       "         [0.4849, 0.4693, 0.5367, 0.4983, 0.4640, 0.4726],\n",
       "         [0.4850, 0.4696, 0.5366, 0.4983, 0.4646, 0.4722],\n",
       "         [0.4853, 0.4694, 0.5367, 0.4982, 0.4648, 0.4724],\n",
       "         [0.4851, 0.4691, 0.5367, 0.4983, 0.4645, 0.4719],\n",
       "         [0.4851, 0.4696, 0.5367, 0.4984, 0.4643, 0.4722],\n",
       "         [0.4849, 0.4694, 0.5366, 0.4977, 0.4645, 0.4724],\n",
       "         [0.4849, 0.4692, 0.5367, 0.4981, 0.4644, 0.4722],\n",
       "         [0.4847, 0.4696, 0.5368, 0.4982, 0.4643, 0.4724],\n",
       "         [0.4852, 0.4697, 0.5368, 0.4986, 0.4640, 0.4724],\n",
       "         [0.4853, 0.4695, 0.5367, 0.4987, 0.4641, 0.4725],\n",
       "         [0.4851, 0.4695, 0.5366, 0.4985, 0.4645, 0.4725],\n",
       "         [0.4848, 0.4696, 0.5367, 0.4979, 0.4644, 0.4723],\n",
       "         [0.4849, 0.4698, 0.5366, 0.4983, 0.4647, 0.4722],\n",
       "         [0.4850, 0.4691, 0.5369, 0.4979, 0.4642, 0.4722],\n",
       "         [0.4848, 0.4695, 0.5369, 0.4983, 0.4645, 0.4726],\n",
       "         [0.4851, 0.4696, 0.5366, 0.4986, 0.4639, 0.4724],\n",
       "         [0.4852, 0.4696, 0.5366, 0.4984, 0.4646, 0.4723],\n",
       "         [0.4850, 0.4698, 0.5365, 0.4988, 0.4648, 0.4721],\n",
       "         [0.4853, 0.4697, 0.5367, 0.4983, 0.4643, 0.4724],\n",
       "         [0.4848, 0.4698, 0.5369, 0.4984, 0.4648, 0.4721],\n",
       "         [0.4851, 0.4695, 0.5365, 0.4988, 0.4647, 0.4723],\n",
       "         [0.4853, 0.4692, 0.5366, 0.4980, 0.4641, 0.4725],\n",
       "         [0.4852, 0.4699, 0.5365, 0.4985, 0.4648, 0.4723],\n",
       "         [0.4847, 0.4697, 0.5368, 0.4984, 0.4644, 0.4724],\n",
       "         [0.4853, 0.4693, 0.5367, 0.4988, 0.4649, 0.4723],\n",
       "         [0.4846, 0.4697, 0.5368, 0.4987, 0.4646, 0.4724],\n",
       "         [0.4850, 0.4692, 0.5366, 0.4981, 0.4645, 0.4722],\n",
       "         [0.4851, 0.4697, 0.5365, 0.4982, 0.4644, 0.4721],\n",
       "         [0.4847, 0.4696, 0.5370, 0.4983, 0.4645, 0.4722],\n",
       "         [0.4849, 0.4697, 0.5367, 0.4981, 0.4640, 0.4721],\n",
       "         [0.4848, 0.4695, 0.5368, 0.4983, 0.4641, 0.4723],\n",
       "         [0.4846, 0.4693, 0.5370, 0.4980, 0.4638, 0.4724],\n",
       "         [0.4855, 0.4700, 0.5365, 0.4983, 0.4645, 0.4722],\n",
       "         [0.4855, 0.4695, 0.5363, 0.4988, 0.4645, 0.4723],\n",
       "         [0.4852, 0.4696, 0.5367, 0.4986, 0.4646, 0.4721],\n",
       "         [0.4851, 0.4698, 0.5367, 0.4983, 0.4641, 0.4719],\n",
       "         [0.4848, 0.4694, 0.5369, 0.4979, 0.4642, 0.4722],\n",
       "         [0.4850, 0.4692, 0.5368, 0.4979, 0.4641, 0.4722],\n",
       "         [0.4847, 0.4694, 0.5368, 0.4979, 0.4639, 0.4724],\n",
       "         [0.4850, 0.4695, 0.5367, 0.4978, 0.4640, 0.4724],\n",
       "         [0.4853, 0.4696, 0.5366, 0.4987, 0.4641, 0.4720],\n",
       "         [0.4852, 0.4694, 0.5365, 0.4982, 0.4645, 0.4723],\n",
       "         [0.4852, 0.4693, 0.5367, 0.4981, 0.4645, 0.4724],\n",
       "         [0.4851, 0.4693, 0.5367, 0.4981, 0.4645, 0.4721]]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pred_boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "595c4b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 7, 16, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pred_masks'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fba6b4",
   "metadata": {},
   "source": [
    "3D: torch.Size([1, 100, 8, 16, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49dbf0c",
   "metadata": {},
   "source": [
    "2D: torch.Size([1, 100, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91380808",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db4f39",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61be4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_cpu = label.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b128ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_image = downsampled_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd5e4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_image = segmented_image.squeeze()\n",
    "if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "classes = segmented_image.unique()\n",
    "colors = [[128,128,128],[255,0,0],[255,255,0],[0,255,0],[0,255,255],[0,0,255],[255,0,255]]\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "for _class in classes[1:]:\n",
    "    points_numpy = (segmented_image == _class).nonzero().numpy()\n",
    "    o3d_point_cloud = o3d.geometry.PointCloud()\n",
    "    o3d_point_cloud.points = o3d.utility.Vector3dVector(points_numpy)\n",
    "    o3d_point_cloud.estimate_normals()\n",
    "    o3d_point_cloud.paint_uniform_color(np.array(colors[int(_class)-1])/255)\n",
    "    vis.add_geometry(o3d_point_cloud)\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd776a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8, 43, 19],\n",
       "       [ 8, 47, 22],\n",
       "       [ 9, 32, 20],\n",
       "       ...,\n",
       "       [19, 38, 24],\n",
       "       [19, 39, 42],\n",
       "       [19, 40, 25]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f717e538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_image.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6667fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The forward expects a NestedTensor, which consists of:\n",
    "       - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "       - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216c643",
   "metadata": {},
   "source": [
    "torch.Size([1, 1, 32, 128, 128]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 4, 4]) torch.Size([1, 4, 4])\n",
    "\n",
    "\n",
    "torch.Size([1, 256, 4, 4])\n",
    "\n",
    "\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "\n",
    "torch.Size([1, 384, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 256, 8, 32, 32]) torch.Size([1, 32, 128, 128]) torch.Size([100, 256]) torch.Size([1, 256, 32, 128, 128])\n",
    "\n",
    "2D:\n",
    "\n",
    "torch.Size([1, 256, 4, 4]) torch.Size([1, 4, 4]) torch.Size([100, 256]) torch.Size([1, 256, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0da2ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_masks(outputs, targets, indices, num_boxes):\n",
    "    \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "       targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
    "    \"\"\"\n",
    "    assert \"pred_masks\" in outputs\n",
    "\n",
    "    src_idx = self._get_src_permutation_idx(indices)\n",
    "    tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "    src_masks = outputs[\"pred_masks\"]\n",
    "    src_masks = src_masks[src_idx]\n",
    "    masks = [t[\"masks\"] for t in targets]\n",
    "    # TODO use valid to mask invalid areas due to padding in loss\n",
    "    target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
    "    target_masks = target_masks.to(src_masks)\n",
    "    target_masks = target_masks[tgt_idx]\n",
    "\n",
    "    # upsample predictions to the target size\n",
    "    src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-3:],\n",
    "                            mode=\"bilinear\", align_corners=False)\n",
    "    src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "    target_masks = target_masks.flatten(1)\n",
    "    target_masks = target_masks.view(src_masks.shape)\n",
    "    losses = {\n",
    "        \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
    "        \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
    "    }\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26fb3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "097dacc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m outputs_without_aux \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maux_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Retrieve the matching between the outputs of the last layer and the targets\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_without_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_resized\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/CHD_Classifier_by_Francisco_Lourenço/models/matcher.py:62\u001b[0m, in \u001b[0;36mHungarianMatcher.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     59\u001b[0m out_bbox \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size * num_queries, 4]\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Also concat the target labels and boxes\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m tgt_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[1;32m     63\u001b[0m tgt_bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Compute the classification cost. Contrary to the loss, we don't use the NLL,\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# but approximate it in 1 - proba[target class].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# The 1 is a constant that doesn't change the matching, it can be ommitted.\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/CHD_Classifier_by_Francisco_Lourenço/models/matcher.py:62\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m out_bbox \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size * num_queries, 4]\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Also concat the target labels and boxes\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m tgt_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[1;32m     63\u001b[0m tgt_bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Compute the classification cost. Contrary to the loss, we don't use the NLL,\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# but approximate it in 1 - proba[target class].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# The 1 is a constant that doesn't change the matching, it can be ommitted.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "# Retrieve the matching between the outputs of the last layer and the targets\n",
    "indices = matcher(outputs_without_aux, label_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "if is_dist_avail_and_initialized():\n",
    "    torch.distributed.all_reduce(num_boxes)\n",
    "num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36be611",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_masks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
