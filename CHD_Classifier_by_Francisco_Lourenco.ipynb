{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR\n",
    "from models.segmentation import DETRsegm\n",
    "from models.matcher import HungarianMatcher\n",
    "from hubconf import detr_resnet101_panoptic, detr_resnet3d_panoptic\n",
    "from torchvision.transforms import Resize\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, target):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(target)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ca9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_downsample_image(image, down_scale = 8):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], down_scale)\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], down_scale)\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], down_scale)\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e187e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmented_image(segmented_image):\n",
    "#     segmented_image.squeeze()\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    colors = [[128,128,128],[255,0,0],[255,255,0],[0,255,0],[0,255,255],[0,0,255],[255,0,255]]\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    for _class in classes[1:]:\n",
    "        points_numpy = (segmented_image == _class).nonzero().numpy()\n",
    "        o3d_point_cloud = o3d.geometry.PointCloud()\n",
    "        o3d_point_cloud.points = o3d.utility.Vector3dVector(points_numpy)\n",
    "        o3d_point_cloud.estimate_normals()\n",
    "        o3d_point_cloud.paint_uniform_color(np.array(colors[int(_class)-1])/255)\n",
    "        vis.add_geometry(o3d_point_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    bb_list = []\n",
    "    for class_ in classes: \n",
    "        points = (segmented_image == 1).nonzero()\n",
    "        x_min, x_max = points[:,0].min(), points[:,0].max()\n",
    "        y_min, y_max = points[:,1].min(), points[:,1].max()\n",
    "        z_min, z_max = points[:,2].min(), points[:,2].max()\n",
    "        bb = torch.tensor([(x_max-x_min)/2, (y_max-y_min)/2, (z_max-z_min)/2, x_max-x_min, y_max-y_min, z_max-z_min])\n",
    "        bb[0::2] = bb[0::2]/torch.tensor(segmented_image.shape)\n",
    "        bb[1::2] = bb[1::2]/torch.tensor(segmented_image.shape)\n",
    "        bb_list.append(bb)\n",
    "    bbs = torch.stack(bb_list)\n",
    "    return bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d8afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    masks_list = []\n",
    "    for class_ in classes[1:]:\n",
    "        mask = segmented_image.clone()\n",
    "        mask[segmented_image == class_] = 1\n",
    "        mask[segmented_image != class_] = 0\n",
    "        masks_list.append(mask)\n",
    "    masks = torch.stack(masks_list)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60020b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(segmented_image):\n",
    "    return segmented_image.unique().int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c56993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_dict(segmented_image):\n",
    "    labels = get_labels(segmented_image)\n",
    "    boxes = get_bounding_boxes(segmented_image)\n",
    "    masks = get_masks(segmented_image)\n",
    "    target_dict = {'labels': labels, 'boxes': boxes, 'masks': masks}\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c93499ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, target_paths, down_scale=8):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.down_scale = down_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array)\n",
    "        downsampled_image = uniform_downsample_image(image_torch_tensor, down_scale=self.down_scale)\n",
    "        target_np_array = nib.load(target_paths[i]).get_fdata()\n",
    "        target_torch_tensor = torch.from_numpy(target_np_array)\n",
    "        downsampled_target = uniform_downsample_image(target_torch_tensor, down_scale=self.down_scale)\n",
    "        target_dict = create_target_dict(target_torch_tensor)\n",
    "        return downsampled_image, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "864b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/francisco/workspace/ImageCHD_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd529519",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob(f'{path}/*image.nii.gz',recursive=True)\n",
    "target_paths = glob(f'{path}/*label.nii.gz',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b202a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths.sort(key=sort_func)\n",
    "target_paths.sort(key=sort_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a76fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = DatasetForSegmentation(image_paths,target_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f118759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-18 09:30:13,371 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-18 09:30:13,966 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    }
   ],
   "source": [
    "image, target = dset[2]\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "914158a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_image = uniform_downsample_image(image)\n",
    "# downsampled_target = uniform_downsample_image()\n",
    "image_reshaped = image.unsqueeze(0).unsqueeze(0).float()\n",
    "# downsampled_label_reshaped = downsampled_label.unsqueeze(0).unsqueeze(0)\n",
    "# im_right_shape = im.transpose(2,0).unsqueeze(0).unsqueeze(0).float()\n",
    "# label_right_shape = label.transpose(2,0).unsqueeze(0).unsqueeze(0).float()\n",
    "# new_size = [int(im_right_shape.shape[2]/8),int(im_right_shape.shape[3]/8),int(im_right_shape.shape[4]/8)]\n",
    "# im_resized = im_right_shape.resize_((1,1,new_size[0],new_size[1],new_size[2]))\n",
    "# label_resized = label_right_shape.resize_((1,1,new_size[0],new_size[1],new_size[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da2874c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detr_seg = detr_resnet3d_panoptic()\n",
    "# detr_seg = detr_resnet11_panoptic()\n",
    "detr_seg.eval();\n",
    "detr_seg.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3190fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = torch.ones((1, 1, 32, 64, 64),device=device)\n",
    "# im = torch.ones((1, 3, 128, 128),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9c0ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/models/position_encoding.py:84: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    }
   ],
   "source": [
    "outputs = detr_seg(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "783f6edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pred_boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "595c4b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pred_logits'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fba6b4",
   "metadata": {},
   "source": [
    "3D: torch.Size([1, 100, 8, 16, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49dbf0c",
   "metadata": {},
   "source": [
    "2D: torch.Size([1, 100, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91380808",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db4f39",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61be4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_cpu = label.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b128ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_image = downsampled_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd776a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8, 43, 19],\n",
       "       [ 8, 47, 22],\n",
       "       [ 9, 32, 20],\n",
       "       ...,\n",
       "       [19, 38, 24],\n",
       "       [19, 39, 42],\n",
       "       [19, 40, 25]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f717e538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_image.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6667fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The forward expects a NestedTensor, which consists of:\n",
    "       - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "       - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216c643",
   "metadata": {},
   "source": [
    "torch.Size([1, 1, 32, 128, 128]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 4, 4]) torch.Size([1, 4, 4])\n",
    "\n",
    "\n",
    "torch.Size([1, 256, 4, 4])\n",
    "\n",
    "\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "\n",
    "torch.Size([1, 384, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 256, 8, 32, 32]) torch.Size([1, 32, 128, 128]) torch.Size([100, 256]) torch.Size([1, 256, 32, 128, 128])\n",
    "\n",
    "2D:\n",
    "\n",
    "torch.Size([1, 256, 4, 4]) torch.Size([1, 4, 4]) torch.Size([100, 256]) torch.Size([1, 256, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da2ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_masks(outputs, targets, indices, num_boxes):\n",
    "    \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "       targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
    "    \"\"\"\n",
    "    assert \"pred_masks\" in outputs\n",
    "\n",
    "    src_idx = self._get_src_permutation_idx(indices)\n",
    "    tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "    src_masks = outputs[\"pred_masks\"]\n",
    "    src_masks = src_masks[src_idx]\n",
    "    masks = [t[\"masks\"] for t in targets]\n",
    "    # TODO use valid to mask invalid areas due to padding in loss\n",
    "    target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
    "    target_masks = target_masks.to(src_masks)\n",
    "    target_masks = target_masks[tgt_idx]\n",
    "\n",
    "    # upsample predictions to the target size\n",
    "    src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-3:],\n",
    "                            mode=\"bilinear\", align_corners=False)\n",
    "    src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "    target_masks = target_masks.flatten(1)\n",
    "    target_masks = target_masks.view(src_masks.shape)\n",
    "    losses = {\n",
    "        \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
    "        \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
    "    }\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26fb3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "097dacc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m outputs_without_aux \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maux_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Retrieve the matching between the outputs of the last layer and the targets\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_without_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/CHD_Classifier_by_Francisco_Lourenço/models/matcher.py:68\u001b[0m, in \u001b[0;36mHungarianMatcher.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     63\u001b[0m tgt_bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Compute the classification cost. Contrary to the loss, we don't use the NLL,\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# but approximate it in 1 - proba[target class].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# The 1 is a constant that doesn't change the matching, it can be ommitted.\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m cost_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mout_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_ids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Compute the L1 cost between boxes\u001b[39;00m\n\u001b[1;32m     71\u001b[0m cost_bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcdist(out_bbox, tgt_bbox, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "# Retrieve the matching between the outputs of the last layer and the targets\n",
    "indices = matcher(outputs_without_aux, [target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "if is_dist_avail_and_initialized():\n",
    "    torch.distributed.all_reduce(num_boxes)\n",
    "num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36be611",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_masks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
