{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98f73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR, LossSegmentation\n",
    "from models.segmentation import DETRsegm, PostProcessPanoptic\n",
    "from models.matcher import HungarianMatcher\n",
    "from hubconf import detr_resnet101_panoptic, detr_resnet3d_panoptic\n",
    "from torchvision.transforms import Resize\n",
    "from util.misc import get_world_size\n",
    "import open3d as o3d\n",
    "import torch.optim as optim\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e667078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.imageglobals.logger.level = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, target):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(target)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ca9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_downsample_image(image, down_scale = 8):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], down_scale)\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], down_scale)\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], down_scale)\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b1ff6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_image_to_given_size(image, size = 128):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], int(image_shape[0]/size))\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], int(image_shape[1]/size))\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], int(image_shape[2]/size))\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e187e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmented_image(segmented_image):\n",
    "#     segmented_image.squeeze()\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    colors = [[128,128,128],[255,0,0],[255,255,0],[0,255,0],[0,255,255],[0,0,255],[255,0,255]]\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    for _class in classes[1:]:\n",
    "        points_numpy = (segmented_image == _class).nonzero().numpy()\n",
    "        o3d_point_cloud = o3d.geometry.PointCloud()\n",
    "        o3d_point_cloud.points = o3d.utility.Vector3dVector(points_numpy)\n",
    "        o3d_point_cloud.estimate_normals()\n",
    "        o3d_point_cloud.paint_uniform_color(np.array(colors[int(_class)-1])/255)\n",
    "        vis.add_geometry(o3d_point_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c45f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    bb_list = []\n",
    "    for class_ in classes[1:]: \n",
    "        points = (segmented_image == class_).nonzero()\n",
    "        x_min, x_max = points[:,0].min(), points[:,0].max()\n",
    "        y_min, y_max = points[:,1].min(), points[:,1].max()\n",
    "        z_min, z_max = points[:,2].min(), points[:,2].max()\n",
    "        bb = torch.tensor([(x_max-x_min)/2, (y_max-y_min)/2, (z_max-z_min)/2, x_max-x_min, y_max-y_min, z_max-z_min])\n",
    "        bb[0::2] = bb[0::2]/torch.tensor(segmented_image.shape)\n",
    "        bb[1::2] = bb[1::2]/torch.tensor(segmented_image.shape)\n",
    "        bb_list.append(bb)\n",
    "    bbs = torch.stack(bb_list)\n",
    "    return bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc5678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    masks_list = []\n",
    "    for class_ in classes[1:]:\n",
    "        mask = segmented_image.clone()\n",
    "        mask[segmented_image == class_] = 1\n",
    "        mask[segmented_image != class_] = 0\n",
    "        masks_list.append(mask.short())\n",
    "    masks = torch.stack(masks_list)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(segmented_image):\n",
    "    return segmented_image.unique().long()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8630212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_dict(segmented_image):\n",
    "    labels = get_labels(segmented_image)\n",
    "    boxes = get_bounding_boxes(segmented_image)\n",
    "    masks = get_masks(segmented_image)\n",
    "    target_dict = {'labels': labels, 'boxes': boxes, 'masks': masks, 'seg_im': segmented_image}\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d489865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_corners(box):\n",
    "    x_min = box[0] - box[3]/2\n",
    "    x_max = box[0] + box[3]/2\n",
    "    y_min = box[1] - box[4]/2\n",
    "    y_max = box[1] + box[4]/2\n",
    "    z_min = box[2] - box[5]/2\n",
    "    z_max = box[2] + box[5]/2\n",
    "    return torch.tensor([x_min,y_min,z_min,x_max,y_max,z_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a57447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersetion_box_corners(box_1,box_2):\n",
    "    intersection_box_corners = torch.zeros(6)\n",
    "    intersection_box_corners[0] = max(box_1[0],box_2[0])\n",
    "    intersection_box_corners[1] = min(box_1[3],box_2[3])\n",
    "    intersection_box_corners[2] = max(box_1[1],box_2[1])\n",
    "    intersection_box_corners[3] = min(box_1[4],box_2[4])\n",
    "    intersection_box_corners[4] = max(box_1[2],box_2[2])\n",
    "    intersection_box_corners[5] = min(box_1[5],box_2[5])\n",
    "    return intersection_box_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74035a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_box_volume(box):\n",
    "    box_volume = (box[3]-box[0])*(box[4]-box[1])*(box[5]-box[2])\n",
    "    if box_volume < 0: box_volume = 0\n",
    "    return box_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c704946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_box_iou(box_1,box_2):\n",
    "    box_1_corners = get_box_corners(box_1)\n",
    "    box_2_corners = get_box_corners(box_2)\n",
    "    intersection_box = get_intersetion_box_corners(box_1_corners,box_2_corners)\n",
    "    intersection_box_volume = compute_box_volume(intersection_box)\n",
    "    if intersection_box_volume == 0: return 0\n",
    "    box_1_volume = compute_box_volume(box_1_corners) \n",
    "    box_2_volume = compute_box_volume(box_2_corners)\n",
    "    iou = intersection_box_volume/(box_1_volume+box_2_volume-intersection_box_volume)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e1eed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_precision(outputs, target, labels):\n",
    "    ious = []\n",
    "    ap = []\n",
    "    for i, label in enumerate(labels):\n",
    "        pred_box = outputs['pred_boxes'].squeeze()[i]\n",
    "        target_box = target['boxes'][label] \n",
    "        iou = compute_box_iou(pred_box,target_box)\n",
    "        ious.append(iou)\n",
    "    ious_tensor = torch.tensor(ious)\n",
    "    for t in range(50,95,5):\n",
    "        tp = (ious_tensor >= t/100).count_nonzero()\n",
    "        fn = abs(len(target['labels']) - 1 - tp)\n",
    "        fp = abs((outputs['pred_logits'].squeeze().argmax(1) != 0).count_nonzero() - 1 - tp)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        ap.append(precision*recall)\n",
    "    ap_tensor = torch.tensor(ap)\n",
    "    return ap_tensor.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa552c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_panoptic_quality(pred_seg, target):\n",
    "    ious = []\n",
    "    for i, label in enumerate(target['labels'][1:]):\n",
    "        target_mask = target['masks'][i] \n",
    "        pred_mask = (pred_seg == label).int()\n",
    "        pred_mask[pred_mask == 0] = -1\n",
    "        intersection = (pred_mask == target_mask).count_nonzero()\n",
    "        union = pred_mask[pred_mask == 1].count_nonzero() + target_mask[target_mask == 1].count_nonzero() - intersection\n",
    "        iou = intersection/union\n",
    "        ious.append(iou)\n",
    "    ious_tensor = torch.tensor(ious)\n",
    "    tp = (ious_tensor > 0.5).count_nonzero()\n",
    "    fn = abs(len(target['labels']) - 1 - tp)\n",
    "    fp = abs(len(pred_seg.unique()) - 1 - tp)\n",
    "    if tp > 0:\n",
    "        sq = ious_tensor[ious_tensor > 0.5].sum() / tp\n",
    "        rq = rq = tp / (tp + fp/2 + fn/2)\n",
    "        pq = sq*rq\n",
    "    else:\n",
    "        sq = 0\n",
    "        rq = 0\n",
    "        pq = 0\n",
    "    return pq, sq, rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a11d132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, target_paths, down_scale=8):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.down_scale = down_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(self.image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array).float()\n",
    "        downsampled_image = uniform_downsample_image(image_torch_tensor, down_scale=self.down_scale)\n",
    "#         downsampled_image = downsample_image_to_given_size(image_torch_tensor, size=64)\n",
    "        target_np_array = nib.load(self.target_paths[i]).get_fdata()\n",
    "        target_torch_tensor = torch.from_numpy(target_np_array)\n",
    "        target_torch_tensor[target_torch_tensor > 7] = 0\n",
    "        downsampled_target = uniform_downsample_image(target_torch_tensor, down_scale=self.down_scale)\n",
    "#         downsampled_target = downsample_image_to_given_size(target_torch_tensor, size=64)\n",
    "        target_dict = create_target_dict(downsampled_target)\n",
    "        return downsampled_image, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "864b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/francisco/workspace/ImageCHD_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd529519",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob(f'{path}/*image.nii.gz',recursive=True)\n",
    "target_paths = glob(f'{path}/*label.nii.gz',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b202a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths.sort(key=sort_func)\n",
    "target_paths.sort(key=sort_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf0e297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DatasetForSegmentation(image_paths,target_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da2874c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = detr_resnet3d_panoptic()\n",
    "# model.eval();\n",
    "# model.half();\n",
    "model = model.detr #To train boxes\n",
    "# model.half();\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f5ba723",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher()\n",
    "loss_segmentation = LossSegmentation(matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45091a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_thing_map = {'0': False, '1': True, '2': True, '3': True, '4': True, '5': True, '6': True, '7': True}\n",
    "post_process_panoptic = PostProcessPanoptic(is_thing_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aceabccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "n_epoch = 25\n",
    "weight_decay = 10e-4\n",
    "ACC_Threshold = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70c4c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b17e0b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25\n",
      "108/108 [================] - 157s 1s/step - Train Loss: 0.6879 - Validation Loss: 0.6756 - Accuracy: 0.0000e+00\n",
      "Epoch: 2/25\n",
      "108/108 [================] - 161s 1s/step - Train Loss: 0.6905 - Validation Loss: 0.6920 - Accuracy: 0.0000e+00\n",
      "Epoch: 3/25\n",
      "108/108 [================] - 160s 1s/step - Train Loss: 0.6978 - Validation Loss: 0.6711 - Accuracy: 0.0000e+00\n",
      "Epoch: 4/25\n",
      "108/108 [================] - 150s 1s/step - Train Loss: 0.6918 - Validation Loss: 0.6877 - Accuracy: 0.0000e+00\n",
      "Epoch: 5/25\n",
      "108/108 [================] - 149s 1s/step - Train Loss: 0.6896 - Validation Loss: 0.6855 - Accuracy: 0.0000e+00\n",
      "Epoch: 6/25\n",
      "108/108 [================] - 157s 1s/step - Train Loss: 0.6874 - Validation Loss: 0.6817 - Accuracy: 0.0000e+00\n",
      "Epoch: 7/25\n",
      "108/108 [================] - 154s 1s/step - Train Loss: 0.6893 - Validation Loss: 0.6857 - Accuracy: 0.0000e+00\n",
      "Epoch: 8/25\n",
      "108/108 [================] - 162s 1s/step - Train Loss: 0.6927 - Validation Loss: 0.6808 - Accuracy: 0.0000e+00\n",
      "Epoch: 9/25\n",
      "108/108 [================] - 164s 2s/step - Train Loss: 0.6924 - Validation Loss: 0.6987 - Accuracy: 0.0000e+00\n",
      "Epoch: 10/25\n",
      "108/108 [================] - 157s 1s/step - Train Loss: 0.6927 - Validation Loss: 0.6719 - Accuracy: 0.0000e+00\n",
      "Epoch: 11/25\n",
      "108/108 [================] - 155s 1s/step - Train Loss: 0.6883 - Validation Loss: 0.6788 - Accuracy: 0.0000e+00\n",
      "Epoch: 12/25\n",
      "108/108 [================] - 156s 1s/step - Train Loss: 0.6904 - Validation Loss: 0.6854 - Accuracy: 0.0000e+00\n",
      "Epoch: 13/25\n",
      "108/108 [================] - 156s 1s/step - Train Loss: 0.6905 - Validation Loss: 0.6710 - Accuracy: 0.0000e+00\n",
      "Epoch: 14/25\n",
      "108/108 [================] - 154s 1s/step - Train Loss: 0.6945 - Validation Loss: 0.6732 - Accuracy: 0.0000e+00\n",
      "Epoch: 15/25\n",
      "108/108 [================] - 155s 1s/step - Train Loss: 0.6877 - Validation Loss: 0.6753 - Accuracy: 0.0000e+00\n",
      "Epoch: 16/25\n",
      "108/108 [================] - 154s 1s/step - Train Loss: 0.6809 - Validation Loss: 0.6728 - Accuracy: 0.0000e+00\n",
      "Epoch: 17/25\n",
      "108/108 [================] - 157s 1s/step - Train Loss: 0.6915 - Validation Loss: 0.6843 - Accuracy: 0.0000e+00\n",
      "Epoch: 18/25\n",
      "108/108 [================] - 153s 1s/step - Train Loss: 0.6869 - Validation Loss: 0.6799 - Accuracy: 0.0000e+00\n",
      "Epoch: 19/25\n",
      "108/108 [================] - 157s 1s/step - Train Loss: 0.6833 - Validation Loss: 0.6744 - Accuracy: 0.0000e+00\n",
      "Epoch: 20/25\n",
      "108/108 [================] - 153s 1s/step - Train Loss: 0.6860 - Validation Loss: 0.6866 - Accuracy: 0.0000e+00\n",
      "Epoch: 21/25\n",
      "108/108 [================] - 153s 1s/step - Train Loss: 0.6870 - Validation Loss: 0.6867 - Accuracy: 0.0000e+00\n",
      "Epoch: 22/25\n",
      "108/108 [================] - 157s 1s/step - Train Loss: 0.6882 - Validation Loss: 0.6887 - Accuracy: 0.0000e+00\n",
      "Epoch: 23/25\n",
      "108/108 [================] - 152s 1s/step - Train Loss: 0.6809 - Validation Loss: 0.6749 - Accuracy: 0.0000e+00\n",
      "Epoch: 24/25\n",
      "108/108 [================] - 147s 1s/step - Train Loss: 0.6853 - Validation Loss: 0.7011 - Accuracy: 0.0000e+00\n",
      "Epoch: 25/25\n",
      "108/108 [================] - 148s 1s/step - Train Loss: 0.6950 - Validation Loss: 0.6817 - Accuracy: 0.0000e+00\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "key = 'loss_bbox'\n",
    "# key = 'loss_mask'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "train_dset_size = 87\n",
    "down_scale = 10\n",
    "\n",
    "valid_dset = DatasetForSegmentation(image_paths[train_dset_size:],target_paths[train_dset_size:], down_scale=down_scale)\n",
    "for epoch in range(n_epoch):\n",
    "    kbar = pkbar.Kbar(target=(train_dset_size+len(valid_dset)-2), epoch=epoch, num_epochs=n_epoch, width=16)\n",
    "    running_loss_t = 0.0\n",
    "    running_loss_v = 0.0\n",
    "    rdm = torch.randperm(train_dset_size)\n",
    "    train_image_paths = [image_paths[i] for i in rdm]\n",
    "    train_target_paths = [target_paths[i] for i in rdm]\n",
    "    train_ds = DatasetForSegmentation(train_image_paths, train_target_paths, down_scale=down_scale)\n",
    "\n",
    "    for idx, b in enumerate(train_ds):\n",
    "        image = b[0]#.half()\n",
    "        image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "        target = b[1]\n",
    "        for i in target.items():\n",
    "#             if i[0] in {'boxes'}: target[i[0]] = i[1].half()\n",
    "            target[i[0]] = i[1].to(device)\n",
    "            \n",
    "        outputs = model(image)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "#         loss = loss_segmentation.loss_masks(outputs, [target]) #To train masks\n",
    "        loss = loss_segmentation.loss_boxes(outputs, [target]) #To train boxes\n",
    "        loss[key].backward() \n",
    "        optimizer.step()\n",
    "        running_loss_t = loss[key].item()\n",
    "        kbar.update(idx, values=[(\"Train Loss\", running_loss_t)])\n",
    "\n",
    "    kbar.add(1, values=[(\"Validation Loss\", 0), (\"Accuracy\", 0)])        \n",
    "    with torch.no_grad():\n",
    "        acc = 0\n",
    "        for ids,b in enumerate(valid_dset):\n",
    "            image = b[0].unsqueeze(0).unsqueeze(0).to(device)\n",
    "            target = b[1]#.half()\n",
    "            for i in target.items():\n",
    "#                 if i[0] in {'boxes'}: target[i[0]] = i[1].half()\n",
    "                target[i[0]] = i[1].to(device)\n",
    "\n",
    "            outputs = model(image)\n",
    "#             loss = loss_segmentation.loss_masks(outputs, [target]) #To train masks\n",
    "            loss = loss_segmentation.loss_boxes(outputs, [target]) #To train boxes\n",
    "            running_loss_v = loss[key].item()\n",
    "            src_idx, trgt_idx = matcher(outputs,[target])[0]\n",
    "#             pred_seg = post_process_panoptic(outputs[trgt_idx],[tuple(torch.tensor(b[0].shape).tolist())]).to(device)\n",
    "#             pq,sq,rq = compute_panoptic_quality(pred_seg, target)\n",
    "#             kbar.update(idx+ids, values=[(\"Validation Loss\", running_loss_v), (\"PQ\", 100*pq),(\"SQ\", 100*sq),(\"RQ\", 100*rq)])\n",
    "            ap = compute_average_precision(outputs, target, trgt_idx) #For boxes\n",
    "            kbar.update(idx+ids, values=[(\"Validation Loss\", running_loss_v), (\"Accuracy\", 100*ap)]) #For boxes\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4f233f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(detr_seg.state_dict(), '/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/chackpoint_lr_10e-4.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dcfad",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97e3bd",
   "metadata": {},
   "source": [
    "# Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "362d1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_thing_map = {'0': False, '1': True, '2': True, '3': True, '4': True, '5': True, '6': True, '7': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4676c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process_panoptic = PostProcessPanoptic(is_thing_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5321fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_masks = outputs['pred_masks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b528de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = post_process_panoptic(outputs,[tuple(torch.tensor(image.squeeze().shape).tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "560c90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_segmented_image(aux_target['seg_im'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e32c933d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 25])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_inpt[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
