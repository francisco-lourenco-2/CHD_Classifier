{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR, LossSegmentation\n",
    "from models.segmentation import DETRsegm, PostProcessPanoptic\n",
    "from models.matcher import HungarianMatcher\n",
    "from hubconf import detr_resnet101_panoptic, detr_resnet3d_panoptic\n",
    "from torchvision.transforms import Resize\n",
    "from util.misc import get_world_size\n",
    "import open3d as o3d\n",
    "import torch.optim as optim\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, target):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(target)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ca9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_downsample_image(image, down_scale = 8):\n",
    "    image_shape = image.shape\n",
    "    dim_0_indexes = torch.arange(0, image_shape[0], down_scale)\n",
    "    dim_1_indexes = torch.arange(0, image_shape[1], down_scale)\n",
    "    dim_2_indexes = torch.arange(0, image_shape[2], down_scale)\n",
    "    downsampled_image = image[dim_0_indexes,:,:]\n",
    "    downsampled_image = downsampled_image[:,dim_1_indexes,:]\n",
    "    downsampled_image = downsampled_image[:,:,dim_2_indexes]\n",
    "    return downsampled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e187e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmented_image(segmented_image):\n",
    "#     segmented_image.squeeze()\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    colors = [[128,128,128],[255,0,0],[255,255,0],[0,255,0],[0,255,255],[0,0,255],[255,0,255]]\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    for _class in classes[1:]:\n",
    "        points_numpy = (segmented_image == _class).nonzero().numpy()\n",
    "        o3d_point_cloud = o3d.geometry.PointCloud()\n",
    "        o3d_point_cloud.points = o3d.utility.Vector3dVector(points_numpy)\n",
    "        o3d_point_cloud.estimate_normals()\n",
    "        o3d_point_cloud.paint_uniform_color(np.array(colors[int(_class)-1])/255)\n",
    "        vis.add_geometry(o3d_point_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    bb_list = []\n",
    "    for class_ in classes: \n",
    "        points = (segmented_image == 1).nonzero()\n",
    "        x_min, x_max = points[:,0].min(), points[:,0].max()\n",
    "        y_min, y_max = points[:,1].min(), points[:,1].max()\n",
    "        z_min, z_max = points[:,2].min(), points[:,2].max()\n",
    "        bb = torch.tensor([(x_max-x_min)/2, (y_max-y_min)/2, (z_max-z_min)/2, x_max-x_min, y_max-y_min, z_max-z_min])\n",
    "        bb[0::2] = bb[0::2]/torch.tensor(segmented_image.shape)\n",
    "        bb[1::2] = bb[1::2]/torch.tensor(segmented_image.shape)\n",
    "        bb_list.append(bb)\n",
    "    bbs = torch.stack(bb_list)\n",
    "    return bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc5678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(segmented_image):\n",
    "    if segmented_image.get_device() > -1: segmented_image = segmented_image.to('cpu')\n",
    "    classes = segmented_image.unique()\n",
    "    masks_list = []\n",
    "    for class_ in classes:\n",
    "        mask = segmented_image.clone()\n",
    "        mask[segmented_image == class_] = 1\n",
    "        mask[segmented_image != class_] = 0\n",
    "        masks_list.append(mask.short())\n",
    "    masks = torch.stack(masks_list)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(segmented_image):\n",
    "    return segmented_image.unique().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8630212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_dict(segmented_image):\n",
    "    labels = get_labels(segmented_image)\n",
    "    boxes = get_bounding_boxes(segmented_image)\n",
    "    masks = get_masks(segmented_image)\n",
    "    target_dict = {'labels': labels, 'boxes': boxes, 'masks': masks, 'seg_im': segmented_image}\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c405d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_panoptic_quality(pred_seg, target):\n",
    "    ious = []\n",
    "    for label in target['labels'][1:]:\n",
    "        target_mask = target['masks'][label] \n",
    "        pred_mask = (pred_seg == label).int()\n",
    "        pred_mask[pred_mask == 0] = -1\n",
    "        intersection = (pred_mask == target_mask).count_nonzero()\n",
    "        union = pred_mask[pred_mask == 1].count_nonzero() + target_mask[target_mask == 1].count_nonzero() - intersection\n",
    "        iou = intersection/union\n",
    "        ious.append(iou)\n",
    "    ious_tensor = torch.tensor(ious)\n",
    "    tp = abs((ious_tensor > 0.5).count_nonzero())\n",
    "    fn = abs(len(target['labels']) - 1 - tp)\n",
    "    fp = abs(len(pred_seg.unique()) - 1 - tp)\n",
    "    if tp > 0:\n",
    "        sq = ious_tensor[ious_tensor > 0.5].sum() / tp\n",
    "        rq = rq = tp / (tp + fp/2 + fn/2)\n",
    "        pq = sq*rq\n",
    "    else:\n",
    "        sq = 0\n",
    "        rq = 0\n",
    "        pq = 0\n",
    "    return pq, sq, rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a11d132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, target_paths, down_scale=8):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.down_scale = down_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array).float()\n",
    "        downsampled_image = uniform_downsample_image(image_torch_tensor, down_scale=self.down_scale)\n",
    "        target_np_array = nib.load(target_paths[i]).get_fdata()\n",
    "        target_torch_tensor = torch.from_numpy(target_np_array)\n",
    "        downsampled_target = uniform_downsample_image(target_torch_tensor, down_scale=self.down_scale)\n",
    "        target_dict = create_target_dict(downsampled_target)\n",
    "        return downsampled_image, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/francisco/workspace/ImageCHD_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd529519",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob(f'{path}/*image.nii.gz',recursive=True)\n",
    "target_paths = glob(f'{path}/*label.nii.gz',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b202a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths.sort(key=sort_func)\n",
    "target_paths.sort(key=sort_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a76fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dset = DatasetForSegmentation(image_paths[:87],target_paths[:87], down_scale=8)\n",
    "# valid_dset = DatasetForSegmentation(image_paths[87:],target_paths[87:], down_scale=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "914158a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_reshaped = image.unsqueeze(0).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da2874c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detr_seg = detr_resnet3d_panoptic()\n",
    "# detr_seg.eval();\n",
    "detr_seg.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c0ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outputs = detr_seg(image_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73ea665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher()\n",
    "loss_segmentation = LossSegmentation(matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "298adece",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_thing_map = {'0': False, '1': True, '2': True, '3': True, '4': True, '5': True, '6': True, '7': True}\n",
    "post_process_panoptic = PostProcessPanoptic(is_thing_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca3dc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "n_epoch = 1\n",
    "weight_decay = 10e-4\n",
    "ACC_Threshold = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a85dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(detr_seg.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e729c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4379ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbar = pkbar.Kbar(target=10)\n",
    "# for i in range(10):\n",
    "#     kbar.update(i)\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e964818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:17:41,153 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:17:41,885 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 0/23 [................] - ETA: 0s - Train Loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:17:48,426 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:17:48,890 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 1/23 [................] - ETA: 4:03 - Train Loss: 0.0470  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:17:52,230 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:17:52,910 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 2/23 [>...............] - ETA: 3:00 - Train Loss: 0.0478"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:17:58,302 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:17:59,512 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 3/23 [=>..............] - ETA: 3:55 - Train Loss: 0.0463"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:18:16,544 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:18:17,494 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 4/23 [=>..............] - ETA: 3:29 - Train Loss: 0.0467"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:18:25,310 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:18:26,243 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 5/23 [==>.............] - ETA: 3:09 - Train Loss: 0.0460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:18:33,778 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:18:34,470 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 6/23 [===>............] - ETA: 2:45 - Train Loss: 0.0456"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:18:39,631 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO - 2022-10-20 16:18:40,533 - batteryrunners - pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for dimension 0 with size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m detr_seg(image)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_segmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/workspace/CHD_Classifier_by_Francisco_Lourenço/models/detr.py:384\u001b[0m, in \u001b[0;36mLossSegmentation.loss_masks\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    381\u001b[0m outputs_without_aux \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maux_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Retrieve the matching between the outputs of the last layer and the targets\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_without_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m num_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets)\n\u001b[1;32m    387\u001b[0m num_boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor([num_boxes], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/CHD_Classifier_by_Francisco_Lourenço/models/matcher.py:69\u001b[0m, in \u001b[0;36mHungarianMatcher.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m tgt_bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Compute the classification cost. Contrary to the loss, we don't use the NLL,\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# but approximate it in 1 - proba[target class].\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# The 1 is a constant that doesn't change the matching, it can be ommitted.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m cost_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mout_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_ids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Compute the L1 cost between boxes\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m cost_bbox \u001b[38;5;241m=\u001b[39m cdist(out_bbox, tgt_bbox)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 15 is out of bounds for dimension 0 with size 9"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "train_dset_size = 2\n",
    "valid_dset = DatasetForSegmentation(image_paths[87:],target_paths[87:], down_scale=8)\n",
    "for epoch in range(n_epoch):\n",
    "    kbar = pkbar.Kbar(target=(train_dset_size+len(valid_dset)-2), epoch=epoch, num_epochs=n_epoch, width=16, stateful_metrics=[\"Accuracy\"])\n",
    "    running_loss_t = 0.0\n",
    "    running_loss_v = 0.0\n",
    "    rdm = torch.randperm(train_dset_size)\n",
    "    train_image_paths = [image_paths[i] for i in rdm]\n",
    "    train_target_paths = [target_paths[i] for i in rdm]\n",
    "    train_ds = DatasetForSegmentation(train_image_paths,train_target_paths, down_scale=8)\n",
    "\n",
    "    for idx, b in enumerate(train_ds):\n",
    "        image = b[0].unsqueeze(0).unsqueeze(0).to(device)\n",
    "        target = b[1]\n",
    "        for i in target.items():\n",
    "            target[i[0]] = i[1].to(device)\n",
    "            \n",
    "        outputs = detr_seg(image)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_segmentation.loss_masks(outputs, [target])\n",
    "        loss['loss_mask'].backward()\n",
    "        optimizer.step()\n",
    "        running_loss_t = loss['loss_mask'].item()\n",
    "        kbar.update(idx, values=[(\"Train Loss\", running_loss_t)])\n",
    "\n",
    "    kbar.add(1, values=[(\"Validation Loss\", 0), (\"Accuracy\", 0)])        \n",
    "    with torch.no_grad():\n",
    "        acc = 0\n",
    "        for ids,b in enumerate(valid_dset):\n",
    "            image = b[0].unsqueeze(0).unsqueeze(0).to(device)\n",
    "            target = b[1]\n",
    "            for i in target.items():\n",
    "                target[i[0]] = i[1].to(device)\n",
    "\n",
    "            outputs = detr_seg(image)\n",
    "            loss = loss_segmentation.loss_masks(outputs, [target])\n",
    "            running_loss_v = loss['loss_mask'].item()\n",
    "            pred_seg = post_process_panoptic(outputs,[tuple(torch.tensor(b[0].shape).tolist())])\n",
    "            pq,sq,rq = compute_panoptic_quality(pred_seg, target)\n",
    "            kbar.update(idx+ids, values=[(\"Validation Loss\", running_loss_v), (\"PQ\", 100*pq),(\"SQ\", 100*sq),(\"RQ\", 100*rq)])\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7121462a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7, 15])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78a9c108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/francisco/workspace/ImageCHD_dataset/ct_1008_image.nii.gz'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_paths[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241d6f3",
   "metadata": {},
   "source": [
    "'/home/francisco/workspace/ImageCHD_dataset/ct_1060_image.nii.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dcfad",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c22002",
   "metadata": {},
   "source": [
    "# Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb55d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_seg = post_process_panoptic(outputs,[tuple(torch.tensor(image.shape).tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b3bd215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 64, 64, 43]), torch.Size([64, 64, 43]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['masks'].shape, pred_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df6b262a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e54a146f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_panoptic_quality(pred_seg, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f411f4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7), tensor(-7), tensor(0))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b1b6bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(2.), tensor(2.))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq, rq, pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccd3ae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ff70945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d45d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "94fbba53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1], dtype=torch.int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mask.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f40515ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(176128)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(aux_pred_mask == target_mask).count_nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97e3bd",
   "metadata": {},
   "source": [
    "# Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "362d1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_thing_map = {'0': False, '1': True, '2': True, '3': True, '4': True, '5': True, '6': True, '7': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4676c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process_panoptic = PostProcessPanoptic(is_thing_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b528de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = post_process_panoptic(outputs,[tuple(torch.tensor(image.shape).tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "560c90ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize_segmented_image(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
