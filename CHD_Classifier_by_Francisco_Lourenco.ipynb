{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR\n",
    "from models.segmentation import DETRsegm\n",
    "from hubconf import detr_resnet101_panoptic, detr_resnet101\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602507f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.position_encoding import PositionEmbeddingSine\n",
    "from util.misc import NestedTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06863b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.hub.resnet import _resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7622b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DETRsegm_3D(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         detr = detr_resnet101_panoptic()\n",
    "#         conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, label):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(label)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9f6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, label_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array)\n",
    "        label_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        label_torch_tensor = torch.from_numpy(label_np_array)\n",
    "        return image_torch_tensor, label_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "864b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/francisco/workspace/ImageCHD_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd529519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = glob(f'{path}/*image.nii.gz',recursive=True)\n",
    "# label_paths = glob(f'{path}/*label.nii.gz',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b202a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths.sort(key=sort_func)\n",
    "# label_paths.sort(key=sort_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a76fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = DatasetForSegmentation(image_paths,label_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f118759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpt, outp = dset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d8e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = _resnet();\n",
    "# model = model.half()\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "914158a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpt_right_shape = inpt.transpose(2,0).unsqueeze(0).unsqueeze(0).float()\n",
    "# new_size = [int(inpt_right_shape.shape[2]/3),int(inpt_right_shape.shape[3]/3),int(inpt_right_shape.shape[4]/3)]\n",
    "# inpt_resized = inpt_right_shape.resize_((1,1,new_size[0],new_size[1],new_size[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4afbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_3d = torch.nn.Conv3d(1,3,(1,3,3),padding = 1)\n",
    "# result = cv_3d(inpt_resized)\n",
    "# result_cuda = result.to(device)\n",
    "# result_2 = model(result_cuda.half())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09bc8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detr = detr_resnet101_panoptic();\n",
    "# detr.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf70a2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da2874c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/francisco/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "detr = detr_resnet101();#_panoptic()\n",
    "detr.eval();\n",
    "detr.to(device);\n",
    "resnet_3d = _resnet();\n",
    "resnet_3d.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3190fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = torch.ones((1, 1, 32, 64, 64),device=device)\n",
    "# im = torch.ones((1, 3, 128, 128),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f9de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detr.backbone[0] = resnet_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3410d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_tensors = NestedTensor(im,torch.ones((2,32,128,128),dtype=torch.int,device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d77cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = PositionEmbeddingSine(num_pos_feats=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5de5434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poss = pos(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9c0ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/models/position_encoding.py:84: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 8, 16, 16]) torch.Size([1, 8, 16, 16]) torch.Size([100, 384]) torch.Size([1, 384, 8, 16, 16])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = detr(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b159f5f",
   "metadata": {},
   "source": [
    "torch.Size([1, 1, 32, 128, 128]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19974763",
   "metadata": {},
   "source": [
    "torch.Size([1, 2048, 4, 4]) torch.Size([1, 4, 4])\n",
    "\n",
    "\n",
    "torch.Size([1, 256, 4, 4])\n",
    "\n",
    "\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "\n",
    "torch.Size([1, 384, 32, 128, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443de0f0",
   "metadata": {},
   "source": [
    "torch.Size([1, 256, 8, 32, 32]) torch.Size([1, 32, 128, 128]) torch.Size([100, 256]) torch.Size([1, 256, 32, 128, 128])\n",
    "\n",
    "2D:\n",
    "\n",
    "torch.Size([1, 256, 4, 4]) torch.Size([1, 4, 4]) torch.Size([100, 256]) torch.Size([1, 256, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595c4b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_logits': tensor([[[ 0.1987, -0.7765, -0.6291,  ..., -0.3947,  0.3182, -0.3763],\n",
       "          [ 0.1879, -0.7659, -0.6195,  ..., -0.4126,  0.3078, -0.3858],\n",
       "          [ 0.1922, -0.7814, -0.6339,  ..., -0.3981,  0.3134, -0.3794],\n",
       "          ...,\n",
       "          [ 0.1962, -0.7872, -0.6320,  ..., -0.3997,  0.3174, -0.3821],\n",
       "          [ 0.1923, -0.7853, -0.6414,  ..., -0.4099,  0.3012, -0.3793],\n",
       "          [ 0.1767, -0.7735, -0.6227,  ..., -0.3992,  0.3092, -0.3966]]],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " 'pred_boxes': tensor([[[0.5040, 0.4942, 0.5530, 0.5042],\n",
       "          [0.5043, 0.4936, 0.5538, 0.5045],\n",
       "          [0.5037, 0.4935, 0.5536, 0.5043],\n",
       "          [0.5036, 0.4937, 0.5543, 0.5046],\n",
       "          [0.5039, 0.4942, 0.5541, 0.5045],\n",
       "          [0.5041, 0.4943, 0.5544, 0.5044],\n",
       "          [0.5035, 0.4941, 0.5541, 0.5045],\n",
       "          [0.5040, 0.4947, 0.5545, 0.5047],\n",
       "          [0.5044, 0.4942, 0.5546, 0.5046],\n",
       "          [0.5045, 0.4935, 0.5543, 0.5045],\n",
       "          [0.5046, 0.4936, 0.5530, 0.5044],\n",
       "          [0.5036, 0.4939, 0.5540, 0.5043],\n",
       "          [0.5041, 0.4941, 0.5530, 0.5043],\n",
       "          [0.5035, 0.4937, 0.5533, 0.5047],\n",
       "          [0.5034, 0.4931, 0.5543, 0.5047],\n",
       "          [0.5038, 0.4932, 0.5544, 0.5050],\n",
       "          [0.5042, 0.4935, 0.5541, 0.5045],\n",
       "          [0.5044, 0.4937, 0.5539, 0.5043],\n",
       "          [0.5041, 0.4939, 0.5547, 0.5043],\n",
       "          [0.5043, 0.4936, 0.5539, 0.5044],\n",
       "          [0.5039, 0.4938, 0.5541, 0.5044],\n",
       "          [0.5037, 0.4939, 0.5538, 0.5051],\n",
       "          [0.5037, 0.4936, 0.5537, 0.5050],\n",
       "          [0.5040, 0.4932, 0.5530, 0.5051],\n",
       "          [0.5044, 0.4929, 0.5525, 0.5050],\n",
       "          [0.5039, 0.4938, 0.5538, 0.5045],\n",
       "          [0.5042, 0.4935, 0.5541, 0.5049],\n",
       "          [0.5039, 0.4938, 0.5539, 0.5049],\n",
       "          [0.5039, 0.4934, 0.5543, 0.5048],\n",
       "          [0.5042, 0.4933, 0.5539, 0.5051],\n",
       "          [0.5042, 0.4941, 0.5548, 0.5047],\n",
       "          [0.5042, 0.4939, 0.5535, 0.5044],\n",
       "          [0.5042, 0.4937, 0.5551, 0.5046],\n",
       "          [0.5031, 0.4927, 0.5530, 0.5048],\n",
       "          [0.5037, 0.4939, 0.5529, 0.5048],\n",
       "          [0.5035, 0.4933, 0.5531, 0.5042],\n",
       "          [0.5036, 0.4935, 0.5536, 0.5049],\n",
       "          [0.5038, 0.4947, 0.5544, 0.5046],\n",
       "          [0.5041, 0.4939, 0.5533, 0.5042],\n",
       "          [0.5036, 0.4939, 0.5539, 0.5039],\n",
       "          [0.5038, 0.4944, 0.5541, 0.5047],\n",
       "          [0.5039, 0.4937, 0.5535, 0.5044],\n",
       "          [0.5040, 0.4945, 0.5539, 0.5043],\n",
       "          [0.5035, 0.4930, 0.5531, 0.5046],\n",
       "          [0.5037, 0.4927, 0.5540, 0.5053],\n",
       "          [0.5036, 0.4936, 0.5537, 0.5043],\n",
       "          [0.5032, 0.4934, 0.5538, 0.5044],\n",
       "          [0.5041, 0.4926, 0.5543, 0.5051],\n",
       "          [0.5041, 0.4934, 0.5545, 0.5045],\n",
       "          [0.5037, 0.4934, 0.5542, 0.5049],\n",
       "          [0.5041, 0.4947, 0.5535, 0.5045],\n",
       "          [0.5039, 0.4942, 0.5525, 0.5046],\n",
       "          [0.5038, 0.4935, 0.5535, 0.5039],\n",
       "          [0.5042, 0.4945, 0.5543, 0.5046],\n",
       "          [0.5040, 0.4934, 0.5540, 0.5045],\n",
       "          [0.5036, 0.4931, 0.5529, 0.5049],\n",
       "          [0.5036, 0.4934, 0.5537, 0.5047],\n",
       "          [0.5039, 0.4928, 0.5526, 0.5049],\n",
       "          [0.5043, 0.4944, 0.5549, 0.5042],\n",
       "          [0.5041, 0.4929, 0.5536, 0.5046],\n",
       "          [0.5039, 0.4942, 0.5546, 0.5041],\n",
       "          [0.5036, 0.4937, 0.5539, 0.5045],\n",
       "          [0.5034, 0.4937, 0.5540, 0.5045],\n",
       "          [0.5043, 0.4944, 0.5535, 0.5041],\n",
       "          [0.5038, 0.4938, 0.5542, 0.5048],\n",
       "          [0.5043, 0.4942, 0.5530, 0.5045],\n",
       "          [0.5038, 0.4927, 0.5534, 0.5046],\n",
       "          [0.5036, 0.4940, 0.5536, 0.5041],\n",
       "          [0.5039, 0.4937, 0.5539, 0.5042],\n",
       "          [0.5043, 0.4939, 0.5541, 0.5046],\n",
       "          [0.5038, 0.4931, 0.5540, 0.5050],\n",
       "          [0.5035, 0.4944, 0.5538, 0.5046],\n",
       "          [0.5045, 0.4938, 0.5538, 0.5041],\n",
       "          [0.5037, 0.4940, 0.5534, 0.5044],\n",
       "          [0.5042, 0.4934, 0.5533, 0.5041],\n",
       "          [0.5038, 0.4932, 0.5536, 0.5049],\n",
       "          [0.5040, 0.4934, 0.5537, 0.5046],\n",
       "          [0.5040, 0.4935, 0.5540, 0.5050],\n",
       "          [0.5044, 0.4929, 0.5538, 0.5044],\n",
       "          [0.5041, 0.4943, 0.5543, 0.5039],\n",
       "          [0.5033, 0.4939, 0.5545, 0.5046],\n",
       "          [0.5039, 0.4931, 0.5528, 0.5052],\n",
       "          [0.5042, 0.4948, 0.5542, 0.5043],\n",
       "          [0.5041, 0.4937, 0.5533, 0.5050],\n",
       "          [0.5045, 0.4937, 0.5534, 0.5049],\n",
       "          [0.5037, 0.4936, 0.5530, 0.5043],\n",
       "          [0.5035, 0.4931, 0.5537, 0.5045],\n",
       "          [0.5043, 0.4942, 0.5544, 0.5042],\n",
       "          [0.5041, 0.4944, 0.5535, 0.5041],\n",
       "          [0.5036, 0.4930, 0.5540, 0.5048],\n",
       "          [0.5035, 0.4934, 0.5535, 0.5045],\n",
       "          [0.5044, 0.4942, 0.5542, 0.5047],\n",
       "          [0.5043, 0.4932, 0.5536, 0.5045],\n",
       "          [0.5035, 0.4939, 0.5541, 0.5044],\n",
       "          [0.5038, 0.4929, 0.5541, 0.5046],\n",
       "          [0.5038, 0.4943, 0.5542, 0.5043],\n",
       "          [0.5038, 0.4943, 0.5535, 0.5041],\n",
       "          [0.5042, 0.4935, 0.5538, 0.5044],\n",
       "          [0.5043, 0.4941, 0.5548, 0.5045],\n",
       "          [0.5038, 0.4930, 0.5531, 0.5044]]], device='cuda:0',\n",
       "        grad_fn=<SelectBackward0>)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91380808",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db4f39",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6667fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The forward expects a NestedTensor, which consists of:\n",
    "       - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "       - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0e3f6fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a more standard version of the position embedding, very similar to the one\n",
    "    used by the Attention is all you need paper, generalized to work on images.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * np.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        x = tensor_list.tensors\n",
    "        mask = tensor_list.mask\n",
    "        assert mask is not None\n",
    "        not_mask = ~mask\n",
    "        z_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        y_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(3, dtype=torch.float32)\n",
    "        \n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            z_embed = z_embed / (z_embed[:, -1:, :, :] + eps) * self.scale\n",
    "            y_embed = y_embed / (y_embed[:, :, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, :, -1:] + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "        \n",
    "        pos_x = x_embed[:, :, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, :, None] / dim_t\n",
    "        pos_z = z_embed[:, :, :, :, None] / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, :, 0::2].sin(), pos_x[:, :, :, :, 1::2].cos()), dim=5).flatten(4)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, :, 0::2].sin(), pos_y[:, :, :, :, 1::2].cos()), dim=5).flatten(4)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=4).permute(0, 4, 1, 2, 3)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c9b262d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55046/3997029984.py:33: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    }
   ],
   "source": [
    "pos = PositionEmbeddingSine(num_pos_feats=128)\n",
    "list_of_tensors = NestedTensor(im,torch.ones((1,32,128,128),dtype=torch.int,device=device))\n",
    "poss = pos(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a77a9419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 32, 128, 128])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
