{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR\n",
    "from models.segmentation import DETRsegm\n",
    "from hubconf import detr_resnet101_panoptic, detr_resnet101\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06863b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.hub.resnet import _resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7622b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DETRsegm_3D(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         detr = detr_resnet101_panoptic()\n",
    "#         conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, label):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(label)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9f6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, label_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array)\n",
    "        label_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        label_torch_tensor = torch.from_numpy(label_np_array)\n",
    "        return image_torch_tensor, label_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "864b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/francisco/workspace/ImageCHD_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd529519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = glob(f'{path}/*image.nii.gz',recursive=True)\n",
    "# label_paths = glob(f'{path}/*label.nii.gz',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b202a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths.sort(key=sort_func)\n",
    "# label_paths.sort(key=sort_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = DatasetForSegmentation(image_paths,label_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f118759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpt, outp = dset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d8e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = _resnet();\n",
    "# model = model.half()\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914158a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpt_right_shape = inpt.transpose(2,0).unsqueeze(0).unsqueeze(0).float()\n",
    "# new_size = [int(inpt_right_shape.shape[2]/3),int(inpt_right_shape.shape[3]/3),int(inpt_right_shape.shape[4]/3)]\n",
    "# inpt_resized = inpt_right_shape.resize_((1,1,new_size[0],new_size[1],new_size[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4afbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_3d = torch.nn.Conv3d(1,3,(1,3,3),padding = 1)\n",
    "# result = cv_3d(inpt_resized)\n",
    "# result_cuda = result.to(device)\n",
    "# result_2 = model(result_cuda.half())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09bc8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detr = detr_resnet101_panoptic();\n",
    "# detr.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf70a2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da2874c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detr_seg = detr_resnet101_panoptic()\n",
    "detr_seg.eval();\n",
    "detr_seg.to(device);\n",
    "resnet_3d = _resnet();\n",
    "resnet_3d.to(device);\n",
    "detr_seg.detr.backbone[0] = resnet_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3190fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = torch.ones((1, 1, 32, 64, 64),device=device)\n",
    "# im = torch.ones((1, 3, 128, 128),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c0ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/models/position_encoding.py:84: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 8, 16, 16])\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mdetr_seg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/CHD_Classifier_by_Francisco_Lourenço/models/segmentation.py:58\u001b[0m, in \u001b[0;36mDETRsegm.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# FIXME h_boxes takes the last one computed, keep this in mind\u001b[39;00m\n\u001b[1;32m     56\u001b[0m bbox_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_attention(hs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], memory, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m---> 58\u001b[0m seg_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_head(src_proj, bbox_mask, [\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtensors, features[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtensors, features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtensors])\n\u001b[1;32m     59\u001b[0m outputs_seg_masks \u001b[38;5;241m=\u001b[39m seg_masks\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetr\u001b[38;5;241m.\u001b[39mnum_queries, seg_masks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], seg_masks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     61\u001b[0m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_masks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs_seg_masks\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "preds = detr_seg(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd2b5c",
   "metadata": {},
   "source": [
    "3D: torch.Size([1, 2048, 8, 16, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ecec31",
   "metadata": {},
   "source": [
    "2D: torch.Size([1, 256, 32, 32]) torch.Size([1, 512, 16, 16]) torch.Size([1, 1024, 8, 8]) torch.Size([1, 2048, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8799028",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['pred_logits'].shape, preds['pred_boxes'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91380808",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db4f39",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6667fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The forward expects a NestedTensor, which consists of:\n",
    "       - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "       - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216c643",
   "metadata": {},
   "source": [
    "torch.Size([1, 1, 32, 128, 128]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 4, 4]) torch.Size([1, 4, 4])\n",
    "\n",
    "\n",
    "torch.Size([1, 256, 4, 4])\n",
    "\n",
    "\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "\n",
    "torch.Size([1, 384, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 256, 8, 32, 32]) torch.Size([1, 32, 128, 128]) torch.Size([100, 256]) torch.Size([1, 256, 32, 128, 128])\n",
    "\n",
    "2D:\n",
    "\n",
    "torch.Size([1, 256, 4, 4]) torch.Size([1, 4, 4]) torch.Size([100, 256]) torch.Size([1, 256, 4, 4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
