{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a98f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from models.detr import DETR\n",
    "from models.segmentation import DETRsegm\n",
    "from hubconf import detr_resnet3d_panoptic\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06863b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.hub.resnet import _resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7622b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DETRsegm_3D(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         detr = detr_resnet101_panoptic()\n",
    "#         conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc7d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(path):\n",
    "    path_id = int(path.split('/')[-1].split('_')[1])\n",
    "    return path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a4badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_label(image, label):\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2, squeeze=False,figsize=(12, 12))\n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Image')\n",
    "    axs[0, 1].imshow(label)\n",
    "    axs[0, 1].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title='Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9f6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSegmentation(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, label_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        image_torch_tensor = torch.from_numpy(image_np_array)\n",
    "        label_np_array = nib.load(image_paths[i]).get_fdata()\n",
    "        label_torch_tensor = torch.from_numpy(label_np_array)\n",
    "        return image_torch_tensor, label_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06485547",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "864b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/francisco/workspace/ImageCHD_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd529519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = glob(f'{path}/*image.nii.gz',recursive=True)\n",
    "# label_paths = glob(f'{path}/*label.nii.gz',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b202a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths.sort(key=sort_func)\n",
    "# label_paths.sort(key=sort_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = DatasetForSegmentation(image_paths,label_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f118759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpt, outp = dset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914158a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpt_right_shape = inpt.transpose(2,0).unsqueeze(0).unsqueeze(0).float()\n",
    "# new_size = [int(inpt_right_shape.shape[2]/3),int(inpt_right_shape.shape[3]/3),int(inpt_right_shape.shape[4]/3)]\n",
    "# inpt_resized = inpt_right_shape.resize_((1,1,new_size[0],new_size[1],new_size[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da2874c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detr_seg = detr_resnet3d_panoptic()\n",
    "detr_seg.eval();\n",
    "detr_seg.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3190fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = torch.ones((1, 1, 32, 64, 64),device=device)\n",
    "# im = torch.ones((1, 3, 128, 128),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c0ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/workspace/CHD_Classifier_by_Francisco_Lourenço/models/position_encoding.py:84: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    }
   ],
   "source": [
    "preds = detr_seg(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595c4b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 8, 16, 16])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds['pred_masks'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91380808",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db4f39",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6667fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The forward expects a NestedTensor, which consists of:\n",
    "       - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "       - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216c643",
   "metadata": {},
   "source": [
    "torch.Size([1, 1, 32, 128, 128]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 2048, 4, 4]) torch.Size([1, 4, 4])\n",
    "\n",
    "\n",
    "torch.Size([1, 256, 4, 4])\n",
    "\n",
    "\n",
    "\n",
    "torch.Size([1, 2048, 8, 32, 32]) torch.Size([1, 32, 128, 128])\n",
    "\n",
    "\n",
    "torch.Size([1, 384, 32, 128, 128])\n",
    "\n",
    "torch.Size([1, 256, 8, 32, 32]) torch.Size([1, 32, 128, 128]) torch.Size([100, 256]) torch.Size([1, 256, 32, 128, 128])\n",
    "\n",
    "2D:\n",
    "\n",
    "torch.Size([1, 256, 4, 4]) torch.Size([1, 4, 4]) torch.Size([100, 256]) torch.Size([1, 256, 4, 4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
